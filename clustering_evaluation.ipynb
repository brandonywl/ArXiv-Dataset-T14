{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47a6ab6b-bdf8-473a-a993-1c76ea873a09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T12:46:53.226667Z",
     "start_time": "2024-04-14T12:46:52.770451Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Clustering evaluation using Rand Index**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "75537e7fecaa8dbe"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b43c1458-0b5e-4687-b274-822c373863d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T12:46:53.372608Z",
     "start_time": "2024-04-14T12:46:53.367371Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# evaluation function with an example\n",
    "# the truths are the list of arxiv categories for each paper, the preds are the cluster labels\n",
    "# rand score modified to allow for multiple categories per paper\n",
    "\n",
    "def modified_rand_score_vectorized(preds, truths):\n",
    "    n = len(preds)\n",
    "    \n",
    "    # Convert predictions to a numpy array for faster operations\n",
    "    preds = np.array(preds)\n",
    "    \n",
    "    # Prepare a label presence matrix\n",
    "    unique_labels = sorted(set(label for sublist in truths for label in sublist))\n",
    "    label_index = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    \n",
    "    # Create a boolean matrix of size (n, number of unique labels)\n",
    "    truth_matrix = np.zeros((n, len(unique_labels)), dtype=bool)\n",
    "    for i, labels in enumerate(truths):\n",
    "        truth_matrix[i, [label_index[label] for label in labels]] = True\n",
    "    \n",
    "    # Calculate pairwise matrix indicating shared label (the slowest part of the function))\n",
    "    shared_label_matrix = np.dot(truth_matrix, truth_matrix.T) > 0\n",
    "    \n",
    "    # Calculate pairwise equal predictions\n",
    "    same_pred = preds[:, None] == preds\n",
    "    \n",
    "    # Calculate TP, TN, FP, FN using vectorized operations\n",
    "    TP = np.sum(np.logical_and(shared_label_matrix, same_pred))\n",
    "    TN = np.sum(np.logical_and(~shared_label_matrix, ~same_pred))\n",
    "    FP = np.sum(np.logical_and(~shared_label_matrix, same_pred))\n",
    "    FN = np.sum(np.logical_and(shared_label_matrix, ~same_pred))\n",
    "\n",
    "    # Correct for self-comparison (diagonal counts as True in both shared_label_matrix and same_pred)\n",
    "    TP -= n\n",
    "    \n",
    "    # Calculate the Rand Index\n",
    "    rand_index = (TP + TN) / (TP + TN + FP + FN) if (TP + TN + FP + FN) > 0 else 0\n",
    "    return rand_index\n",
    "\n",
    "# Example usage\n",
    "preds = [1, 1, 2, 2]\n",
    "truths = [['a', 'b'], ['a'], ['c'], ['c', 'd']]\n",
    "print(modified_rand_score_vectorized(preds, truths))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Example usage with arxiv data**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c9cc4c6fb2a301e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# function to add back the list of categories for each paper\n",
    "def reorg_category_df(df_categories):\n",
    "    data_ids, data_categories = [], []\n",
    "    cur_id, cat = \"\", []\n",
    "    data = df_categories.sort_values(\"id\")\n",
    "\n",
    "    for _, row in tqdm(data.iterrows(), total=data.shape[0]):\n",
    "        if cur_id != row[0]:\n",
    "            # save the exising id's categories data to dictionary. if cur_id is \"\", it indicates the beginning of the loop, no prior records to save\n",
    "            if cur_id != \"\":\n",
    "                data_ids.append(cur_id)\n",
    "                data_categories.append(\",\".join(cat))\n",
    "                cur_id = row[0]\n",
    "                cat = []\n",
    "            else:\n",
    "                cur_id = row[0]\n",
    "        cat.append(row[1])\n",
    "\n",
    "    df = pd.DataFrame.from_dict({\"id\": data_ids, 'categories': data_categories})\n",
    "    # save it to csv, so you could load it next time without rerunning\n",
    "    # df.to_csv(save_to_file_name)\n",
    "    # print(f\"Total {df.shape[0]} records. Have saved the file {save_to_file_name} into the data folder.\")\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-14T12:46:53.492669Z",
     "start_time": "2024-04-14T12:46:53.488235Z"
    }
   },
   "id": "63ccd8db59feafc2",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4156055/4156055 [01:19<00:00, 52487.84it/s]\n"
     ]
    }
   ],
   "source": [
    "# the data/arxiv-metadata-ext-category.csv file contains N rows of the same paper (same paper id) with N different categories\n",
    "df_categories = pd.read_csv(\"data/arxiv-metadata-ext-category.csv\",dtype={\"id\":object,\"category_id\":object})\n",
    "# Reorganize to each row as one paper with a column containing the list of categories. Might take 1 to 3+ mins.\n",
    "df_reorg_category = reorg_category_df(df_categories)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-14T12:48:17.047353Z",
     "start_time": "2024-04-14T12:46:53.573456Z"
    }
   },
   "id": "a1824196c8f2b8a6",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# load the clustering results with at least two columns: id and kmeans_label\n",
    "df_clustering = pd.read_csv(\"filename.csv\",dtype={\"id\":object})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-14T12:48:17.347275Z",
     "start_time": "2024-04-14T12:48:17.053425Z"
    }
   },
   "id": "5f490810c4bb2bf8",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_clustering = df_clustering.merge(df_reorg_category, how='left', on=\"id\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-14T12:48:19.314736Z",
     "start_time": "2024-04-14T12:48:17.348736Z"
    }
   },
   "id": "a6f64488d79aa50c",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1fd822e-bab9-4c2f-a5c5-f3bc973a5b7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T12:48:19.315279Z",
     "start_time": "2024-04-14T12:48:19.314230Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_comma(input):\n",
    "    try:\n",
    "        return input.split(\",\")\n",
    "    except:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cfcb513-553e-4e14-8a5e-72d02259d125",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T12:48:20.088409Z",
     "start_time": "2024-04-14T12:48:19.317797Z"
    }
   },
   "outputs": [],
   "source": [
    "df_clustering[\"categories_list\"] = df_clustering[\"categories\"].apply(lambda x: split_comma(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6de7405e-8422-4ab9-a43f-4fb3c7883c54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T12:50:05.277363Z",
     "start_time": "2024-04-14T12:48:20.114309Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 0, score: 0.7954620262026203\n",
      "seed: 1, score: 0.7983003700370037\n",
      "seed: 2, score: 0.7919541354135413\n",
      "seed: 3, score: 0.7935656565656566\n",
      "seed: 4, score: 0.7985075707570757\n",
      "seed: 5, score: 0.7984138613861386\n",
      "seed: 6, score: 0.7968819681968197\n",
      "seed: 7, score: 0.7975040504050405\n",
      "seed: 8, score: 0.7983177117711772\n",
      "seed: 9, score: 0.7969073307330733\n",
      "Average Rand Index Score\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.7965814681468146"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_score = []\n",
    "# to speed up the evaluation, we sample random 10,000 papers 10 times and evaluate the clustering results based on the average score\n",
    "for i in range(10):\n",
    "    df_clustering_filter = df_clustering[~df_clustering[\"categories\"].isna()]\n",
    "    df_sample = df_clustering_filter.sample(10000, random_state = i)\n",
    "    score = modified_rand_score_vectorized(df_sample[\"kmeans_label\"], df_sample[\"categories_list\"])\n",
    "    output_score.append(score)\n",
    "    print(f\"seed: {i}, score: {score}\")\n",
    "    \n",
    "print(f\"Average Rand Index Score: {np.mean(output_score)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Clustering evaluation using Silhouette Score (if possible)**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a919526606277e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# X is a feature array and labels are predicted labels for each sample\n",
    "# reference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html\n",
    "silhouette_score(X, labels)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ad9fdb3ef365113"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
