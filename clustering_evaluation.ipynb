{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47a6ab6b-bdf8-473a-a993-1c76ea873a09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T07:33:10.758562Z",
     "start_time": "2024-04-16T07:33:09.302702Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75537e7fecaa8dbe",
   "metadata": {},
   "source": [
    "## Clustering evaluation using Rand Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b43c1458-0b5e-4687-b274-822c373863d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T07:33:10.785073Z",
     "start_time": "2024-04-16T07:33:10.651882Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# evaluation function with an example\n",
    "# the truths are the list of arxiv categories for each paper, the preds are the cluster labels\n",
    "# rand score modified to allow for multiple categories per paper\n",
    "\n",
    "def modified_rand_score_vectorized(preds, truths):\n",
    "    n = len(preds)\n",
    "    \n",
    "    # Convert predictions to a numpy array for faster operations\n",
    "    preds = np.array(preds)\n",
    "    \n",
    "    # Prepare a label presence matrix\n",
    "    unique_labels = sorted(set(label for sublist in truths for label in sublist))\n",
    "    label_index = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    \n",
    "    # Create a boolean matrix of size (n, number of unique labels)\n",
    "    truth_matrix = np.zeros((n, len(unique_labels)), dtype=bool)\n",
    "    for i, labels in enumerate(truths):\n",
    "        truth_matrix[i, [label_index[label] for label in labels]] = True\n",
    "    \n",
    "    # Calculate pairwise matrix indicating shared label (the slowest part of the function))\n",
    "    shared_label_matrix = np.dot(truth_matrix, truth_matrix.T) > 0\n",
    "    \n",
    "    # Calculate pairwise equal predictions\n",
    "    same_pred = preds[:, None] == preds\n",
    "    \n",
    "    # Calculate TP, TN, FP, FN using vectorized operations\n",
    "    TP = np.sum(np.logical_and(shared_label_matrix, same_pred))\n",
    "    TN = np.sum(np.logical_and(~shared_label_matrix, ~same_pred))\n",
    "    FP = np.sum(np.logical_and(~shared_label_matrix, same_pred))\n",
    "    FN = np.sum(np.logical_and(shared_label_matrix, ~same_pred))\n",
    "\n",
    "    # Correct for self-comparison (diagonal counts as True in both shared_label_matrix and same_pred)\n",
    "    TP -= n\n",
    "    \n",
    "    # Calculate the Rand Index\n",
    "    rand_index = (TP + TN) / (TP + TN + FP + FN) if (TP + TN + FP + FN) > 0 else 0\n",
    "    return rand_index\n",
    "\n",
    "# Example usage\n",
    "preds = [1, 1, 2, 2]\n",
    "truths = [['a', 'b'], ['a'], ['c'], ['c', 'd']]\n",
    "print(modified_rand_score_vectorized(preds, truths))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a663ed9cb6a3341",
   "metadata": {},
   "source": [
    "##  Example usage with arxiv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63ccd8db59feafc2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T07:33:10.841448Z",
     "start_time": "2024-04-16T07:33:10.664077Z"
    }
   },
   "outputs": [],
   "source": [
    "# function to add back the list of categories for each paper\n",
    "def reorg_category_df(df_categories):\n",
    "    data_ids, data_categories = [], []\n",
    "    cur_id, cat = \"\", []\n",
    "    data = df_categories.sort_values(\"id\")\n",
    "\n",
    "    for _, row in tqdm(data.iterrows(), total=data.shape[0]):\n",
    "        if cur_id != row[0]:\n",
    "            # save the exising id's categories data to dictionary. if cur_id is \"\", it indicates the beginning of the loop, no prior records to save\n",
    "            if cur_id != \"\":\n",
    "                data_ids.append(cur_id)\n",
    "                data_categories.append(\",\".join(cat))\n",
    "                cur_id = row[0]\n",
    "                cat = []\n",
    "            else:\n",
    "                cur_id = row[0]\n",
    "        cat.append(row[1])\n",
    "\n",
    "    df = pd.DataFrame.from_dict({\"id\": data_ids, 'categories': data_categories})\n",
    "    # save it to csv, so you could load it next time without rerunning\n",
    "    # df.to_csv(save_to_file_name)\n",
    "    # print(f\"Total {df.shape[0]} records. Have saved the file {save_to_file_name} into the data folder.\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1824196c8f2b8a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T07:34:36.806243Z",
     "start_time": "2024-04-16T07:33:10.673644Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████▊                  | 1644358/4156055 [00:47<01:11, 35003.80it/s]"
     ]
    }
   ],
   "source": [
    "# the data/arxiv-metadata-ext-category.csv file contains N rows of the same paper (same paper id) with N different categories\n",
    "df_categories = pd.read_csv(\"data/arxiv-metadata-ext-category.csv\",dtype={\"id\":object,\"category_id\":object})\n",
    "# Reorganize to each row as one paper with a column containing the list of categories. Might take 1 to 3+ mins.\n",
    "df_reorg_category = reorg_category_df(df_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f490810c4bb2bf8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T09:09:07.121009Z",
     "start_time": "2024-04-16T09:09:07.076229Z"
    }
   },
   "outputs": [],
   "source": [
    "# load the clustering results with at least two columns: id and kmeans_label\n",
    "df_clustering = pd.read_csv(\"data/bertopic-kmeans60.csv\",dtype={\"id\":object})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f64488d79aa50c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T09:09:08.414402Z",
     "start_time": "2024-04-16T09:09:07.510781Z"
    }
   },
   "outputs": [],
   "source": [
    "df_clustering = df_clustering.merge(df_reorg_category, how='left', on=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d853a4969e4633",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T09:09:08.425043Z",
     "start_time": "2024-04-16T09:09:08.414597Z"
    }
   },
   "outputs": [],
   "source": [
    "df_clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fd822e-bab9-4c2f-a5c5-f3bc973a5b7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T09:09:08.426074Z",
     "start_time": "2024-04-16T09:09:08.419406Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_comma(input):\n",
    "    try:\n",
    "        return input.split(\",\")\n",
    "    except:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfcb513-553e-4e14-8a5e-72d02259d125",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T09:09:08.482216Z",
     "start_time": "2024-04-16T09:09:08.424116Z"
    }
   },
   "outputs": [],
   "source": [
    "df_clustering[\"categories_list\"] = df_clustering[\"categories\"].apply(lambda x: split_comma(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de7405e-8422-4ab9-a43f-4fb3c7883c54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T09:10:53.980900Z",
     "start_time": "2024-04-16T09:09:16.533847Z"
    }
   },
   "outputs": [],
   "source": [
    "output_score = []\n",
    "# to speed up the evaluation, we sample random 10,000 papers 10 times and evaluate the clustering results based on the average score\n",
    "for i in range(10):\n",
    "    df_clustering_filter = df_clustering[~df_clustering[\"categories\"].isna()]\n",
    "    df_sample = df_clustering_filter.sample(10000, random_state = i)\n",
    "    #score = modified_rand_score_vectorized(df_sample[\"kmeans_label\"], df_sample[\"categories_list\"])\n",
    "    score = modified_rand_score_vectorized(df_sample[\"topic_id\"], df_sample[\"categories_list\"])\n",
    "    output_score.append(score)\n",
    "    print(f\"seed: {i}, score: {score}\")\n",
    "    \n",
    "print(f\"Average Rand Index Score: {np.mean(output_score)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a919526606277e",
   "metadata": {},
   "source": [
    "## Clustering evaluation using Silhouette Score (if possible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad9fdb3ef365113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X is a feature array and labels are predicted labels for each sample\n",
    "# reference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html\n",
    "# silhouette_score(X, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
