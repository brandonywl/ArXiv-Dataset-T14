{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64c07e3c-f41d-41b0-b619-34d75362f8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "\n",
    "# from src.data_handler.dataloader import load_citations\n",
    "from get_clean_text import load_cs_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "035a4ff9-2c5e-43aa-bb24-2ad60f1a413f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cs papers\n",
      "cs papers loaded\n",
      "Executing clean\n",
      "Done\n",
      "Executing abbv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/604853 [00:00<?, ?it/s]c:\\Users\\Stefa\\Downloads\\ArXiv-Dataset-T14\\src\\scispacy\\abbreviation.py:260: UserWarning: [W036] The component 'matcher' does not have any patterns defined.\n",
      "  global_matches = self.global_matcher(doc)\n",
      "100%|██████████| 604853/604853 [16:31<00:00, 610.05it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "papers = load_cs_papers(\"\", \"clean_abbv\", run_preprocessor=True, to_exclude=[\"case-fold\", \"punct-removal\", \"tokenize\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1c6b36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cs papers\n",
      "cs papers loaded\n"
     ]
    }
   ],
   "source": [
    "papers = load_cs_papers(\"clean_abbv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e995de96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cs papers\n",
      "cs papers loaded\n",
      "Executing case-fold\n",
      "Done\n",
      "Executing punct-removal\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "papers = load_cs_papers(\"clean_abbv\", \"clean_abbv_casefold_punct\", run_preprocessor=True, to_exclude=[\"clean\", \"abbv\", \"tokenize\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b39215b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cs papers\n",
      "cs papers loaded\n",
      "Executing tokenize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 604853/604853 [09:13<00:00, 1093.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "papers = load_cs_papers(\"clean_abbv_casefold_punct\", \"clean_abbv_casefold_punct_spacytokenizer\", run_preprocessor=True, to_include=[\"tokenize\"], no_stop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f63a477f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cs papers\n",
      "cs papers loaded\n"
     ]
    }
   ],
   "source": [
    "papers = load_cs_papers(\"clean_abbv_casefold_punct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ed50b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b2849aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(preprocessor=lambda x: x, tokenizer=lambda x:x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2820ad5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Stefa\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "abstract_vec = vec.fit_transform(papers['abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cdc16a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<604853x666828 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 60857131 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b646b1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<604853x666828 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 60857131 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "102a16b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers.modeling_bert import BertModel, BertForMaskedLM\n",
    "from transformers import BertTokenizerFast, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f205e3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "# encoded_input = tokenizer(text, return_tensors='pt')\n",
    "# output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b4ed3892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b30113cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = papers.abstract.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8b938fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9b203a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 26.89it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "batches = int(np.ceil(len(abstracts) / batch_size))\n",
    "batches\n",
    "\n",
    "tokens = []\n",
    "\n",
    "for i in tqdm(range(20)):\n",
    "    abstracts_ = abstracts[i * batch_size: (i+1) * batch_size]\n",
    "    encoded_input = tokenizer(abstracts_, return_tensors='pt', padding=\"max_length\", truncation=True, max_length=512)\n",
    "    tokens.append(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3ac95830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 2057, 6235,  ...,    0,    0,    0],\n",
       "        [ 101, 1999, 1037,  ...,    0,    0,    0],\n",
       "        [ 101, 1996, 9414,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 101, 1996, 2465,  ...,    0,    0,    0],\n",
       "        [ 101, 2592, 8346,  ...,    0,    0,    0],\n",
       "        [ 101, 1996, 2591,  ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5d8d635a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [07:29<42:28, 149.90s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[106], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tqdm(tokens):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 4\u001b[0m         output \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtoken)\n\u001b[0;32m      5\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n",
      "File \u001b[1;32mc:\\Users\\Stefa\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Stefa\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Stefa\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1004\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1006\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m   1007\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1008\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1011\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1012\u001b[0m )\n\u001b[1;32m-> 1013\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m   1014\u001b[0m     embedding_output,\n\u001b[0;32m   1015\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   1016\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   1017\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m   1018\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_extended_attention_mask,\n\u001b[0;32m   1019\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m   1020\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m   1021\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1022\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1023\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1024\u001b[0m )\n\u001b[0;32m   1025\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1026\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Stefa\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Stefa\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Stefa\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    596\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    597\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    598\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    604\u001b[0m         output_attentions,\n\u001b[0;32m    605\u001b[0m     )\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 607\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[0;32m    608\u001b[0m         hidden_states,\n\u001b[0;32m    609\u001b[0m         attention_mask,\n\u001b[0;32m    610\u001b[0m         layer_head_mask,\n\u001b[0;32m    611\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    612\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    613\u001b[0m         past_key_value,\n\u001b[0;32m    614\u001b[0m         output_attentions,\n\u001b[0;32m    615\u001b[0m     )\n\u001b[0;32m    617\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\Stefa\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Stefa\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Stefa\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:539\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    536\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    537\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 539\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m apply_chunking_to_forward(\n\u001b[0;32m    540\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward_chunk, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_size_feed_forward, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len_dim, attention_output\n\u001b[0;32m    541\u001b[0m )\n\u001b[0;32m    542\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    544\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Stefa\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\transformers\\pytorch_utils.py:237\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m forward_fn(\u001b[38;5;241m*\u001b[39minput_tensors)\n",
      "File \u001b[1;32mc:\\Users\\Stefa\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:551\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m--> 551\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m    552\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32mc:\\Users\\Stefa\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Stefa\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Stefa\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:451\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 451\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[0;32m    452\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\Stefa\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Stefa\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Stefa\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "for token in tqdm(tokens):\n",
    "    with torch.no_grad():\n",
    "        output = model(**token)\n",
    "    outputs.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3926d3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = [output.last_hidden_state.numpy() for output in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9fc21427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.5287565 , -0.01068921, -0.499239  , ..., -0.41446254,\n",
       "         0.02744966,  0.30988243],\n",
       "       [-0.5717407 , -0.40985718, -0.27441454, ..., -0.23279467,\n",
       "         0.14650892,  0.52955323],\n",
       "       [-0.69186205, -0.16229591,  0.07731377, ..., -0.29492173,\n",
       "        -0.20344765,  0.6825587 ],\n",
       "       ...,\n",
       "       [-0.83634704, -0.20692849, -0.01469363, ..., -0.47055644,\n",
       "        -0.06038333,  0.32870397],\n",
       "       [-0.59632474, -0.42031935, -0.01748319, ..., -0.1508324 ,\n",
       "         0.07424235,  0.49122122],\n",
       "       [-0.6390631 , -0.18198538, -0.25880927, ..., -0.77206844,\n",
       "         0.11244365, -0.15346798]], dtype=float32)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_vecs = [output[:,0,:] for output in outputs]\n",
    "global_vecs = np.concatenate(global_vecs)\n",
    "global_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b1e14901",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import cosine_sim, top_k_retrieval\n",
    "import src.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "0ae5f282",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "9b2fd8f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                     0704.0002\n",
       "title                   Sparsity-certifying Graph Decompositions\n",
       "abstract       we describe a new algorithm the [eqn_latex]peb...\n",
       "authors                          Ileana Streinu and Louis Theran\n",
       "journal-ref                                                  NaN\n",
       "license        http://arxiv.org/licenses/nonexclusive-distrib...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "16214d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Options for Project Schedules (ROPS) 0704.0090\n",
      "[0.87688315 0.86930764 0.85959297 0.8586812  0.85861486]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal-ref</th>\n",
       "      <th>license</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0704.0860</td>\n",
       "      <td>Availability assessment of SunOS/Solaris Unix ...</td>\n",
       "      <td>this paper presents a measurementbased availab...</td>\n",
       "      <td>Cristina Simache (LAAS), Mohamed Kaaniche (LAAS)</td>\n",
       "      <td>Proc. 2005 IEEE Pacific Rim International Symp...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0704.1925</td>\n",
       "      <td>Blind Identification of Distributed Antenna Sy...</td>\n",
       "      <td>in spatially distributed multiuser antenna sys...</td>\n",
       "      <td>Yuanning Yu, Athina P. Petropulu and H. Vincen...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0704.2725</td>\n",
       "      <td>Exploiting Heavy Tails in Training Times of Mu...</td>\n",
       "      <td>the random initialization of weights of a mult...</td>\n",
       "      <td>Manuel Cebrian and Ivan Cantador</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0704.1833</td>\n",
       "      <td>Analysis of the 802.11e Enhanced Distributed C...</td>\n",
       "      <td>the ieee e standard revises the medium access ...</td>\n",
       "      <td>Inanc Inan, Feyza Keceli, Ender Ayanoglu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0704.2383</td>\n",
       "      <td>Power control and receiver design for energy e...</td>\n",
       "      <td>this paper is focused on the crosslayer design...</td>\n",
       "      <td>Stefano Buzzi, Valeria Massaro, and H. Vincent...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                              title  \\\n",
       "32   0704.0860  Availability assessment of SunOS/Solaris Unix ...   \n",
       "81   0704.1925  Blind Identification of Distributed Antenna Sy...   \n",
       "115  0704.2725  Exploiting Heavy Tails in Training Times of Mu...   \n",
       "76   0704.1833  Analysis of the 802.11e Enhanced Distributed C...   \n",
       "97   0704.2383  Power control and receiver design for energy e...   \n",
       "\n",
       "                                              abstract  \\\n",
       "32   this paper presents a measurementbased availab...   \n",
       "81   in spatially distributed multiuser antenna sys...   \n",
       "115  the random initialization of weights of a mult...   \n",
       "76   the ieee e standard revises the medium access ...   \n",
       "97   this paper is focused on the crosslayer design...   \n",
       "\n",
       "                                               authors  \\\n",
       "32    Cristina Simache (LAAS), Mohamed Kaaniche (LAAS)   \n",
       "81   Yuanning Yu, Athina P. Petropulu and H. Vincen...   \n",
       "115                   Manuel Cebrian and Ivan Cantador   \n",
       "76            Inanc Inan, Feyza Keceli, Ender Ayanoglu   \n",
       "97   Stefano Buzzi, Valeria Massaro, and H. Vincent...   \n",
       "\n",
       "                                           journal-ref license  \n",
       "32   Proc. 2005 IEEE Pacific Rim International Symp...     NaN  \n",
       "81                                                 NaN     NaN  \n",
       "115                                                NaN     NaN  \n",
       "76                                                 NaN     NaN  \n",
       "97                                                 NaN     NaN  "
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 5\n",
    "print(papers.iloc[i].title, papers.iloc[i].id)\n",
    "print(sims[i][:,1])\n",
    "papers.iloc[sims[i][:,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "1fc9a657",
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = src.utils.top_k_retrieval(cosine_sim(global_vecs), top_k=5, return_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0c712a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_vec(text, top_k=1, return_scores=True, paper_subset=None, preprocessor=None, tokenizer=None, model=None):\n",
    "    paper_subset_vecs = paper_subset\n",
    "    if preprocessor is not None:\n",
    "        text = preprocessor(text)\n",
    "    search_tokens = tokenizer(text, return_tensors='pt', padding=\"max_length\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        output = model(**search_tokens)\n",
    "    search_vec = output.last_hidden_state.numpy()[:, 0, :]\n",
    "    sim_papers = src.utils.top_k_retrieval(cosine_sim(search_vec, paper_subset_vecs), top_k=top_k, return_score=True)\n",
    "    sim_papers_index = sim_papers[:, 0]\n",
    "    sim_papers_score = sim_papers[:, 1]\n",
    "    \n",
    "    sim_papers = paper_subset.iloc[sim_papers_index]\n",
    "    if return_scores:\n",
    "        sim_papers.loc[:, \"similarity_score\"] = sim_papers_score\n",
    "\n",
    "    return sim_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "f443b8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Adam a method for stochastic optimization\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "13cad18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_search = tokenizer(text, return_tensors='pt', padding=\"max_length\", truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "2eb0d265",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(**inp_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "863a85f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_vec = output.last_hidden_state.numpy()[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "7dd9a14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_papers = src.utils.top_k_retrieval(cosine_sim(search_vec, global_vecs), top_k=10, return_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "d58ffb18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal-ref</th>\n",
       "      <th>license</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0704.0108</td>\n",
       "      <td>Reducing SAT to 2-SAT</td>\n",
       "      <td>description of a polynomial time reduction of ...</td>\n",
       "      <td>Sergey Gubin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0704.3157</td>\n",
       "      <td>Experimenting with recursive queries in databa...</td>\n",
       "      <td>this paper considers the problem of reasoning ...</td>\n",
       "      <td>Giorgio Terracina, Nicola Leone, Vincenzino Li...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0704.1756</td>\n",
       "      <td>The Invar Tensor Package</td>\n",
       "      <td>the invar package is introduced a fast manipul...</td>\n",
       "      <td>Jose M. Martin-Garcia, Renato Portugal, Leon R...</td>\n",
       "      <td>Comp. Phys. Commun. 177 (2007) 640-648</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0704.2779</td>\n",
       "      <td>The Complexity of Simple Stochastic Games</td>\n",
       "      <td>in this paper we survey the computational time...</td>\n",
       "      <td>Jonas Dieckelmann</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0704.0282</td>\n",
       "      <td>On Punctured Pragmatic Space-Time Codes in Blo...</td>\n",
       "      <td>this paper considers the use of punctured conv...</td>\n",
       "      <td>Samuele Bandi, Luca Stabellini, Andrea Conti a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0704.2351</td>\n",
       "      <td>Parallel computation of the rank of large spar...</td>\n",
       "      <td>this paper deals with the computation of the r...</td>\n",
       "      <td>Jean-Guillaume Dumas (LMC - IMAG), Philippe El...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0704.2857</td>\n",
       "      <td>Modern Coding Theory: The Statistical Mechanic...</td>\n",
       "      <td>these are the notes for a set of lectures deli...</td>\n",
       "      <td>Andrea Montanari and Rudiger Urbanke</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0704.0098</td>\n",
       "      <td>Sparsely-spread CDMA - a statistical mechanics...</td>\n",
       "      <td>sparse code division multiple access a variati...</td>\n",
       "      <td>Jack Raymond, David Saad</td>\n",
       "      <td>J. Phys. A: Math. Theor. 40 No 41 (12 October ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0704.2596</td>\n",
       "      <td>Computing Extensions of Linear Codes</td>\n",
       "      <td>this paper deals with the problem of increasin...</td>\n",
       "      <td>Markus Grassl</td>\n",
       "      <td>Proceedings 2007 IEEE International Symposium ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>0704.3780</td>\n",
       "      <td>Stochastic Optimization Algorithms</td>\n",
       "      <td>when looking for a solution deterministic meth...</td>\n",
       "      <td>Pierre Collet, Jean-Philippe Rennard</td>\n",
       "      <td>Rennard, J.-P., Handbook of Research on Nature...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                              title  \\\n",
       "7    0704.0108                              Reducing SAT to 2-SAT   \n",
       "134  0704.3157  Experimenting with recursive queries in databa...   \n",
       "70   0704.1756                           The Invar Tensor Package   \n",
       "117  0704.2779          The Complexity of Simple Stochastic Games   \n",
       "12   0704.0282  On Punctured Pragmatic Space-Time Codes in Blo...   \n",
       "93   0704.2351  Parallel computation of the rank of large spar...   \n",
       "123  0704.2857  Modern Coding Theory: The Statistical Mechanic...   \n",
       "6    0704.0098  Sparsely-spread CDMA - a statistical mechanics...   \n",
       "108  0704.2596               Computing Extensions of Linear Codes   \n",
       "180  0704.3780                 Stochastic Optimization Algorithms   \n",
       "\n",
       "                                              abstract  \\\n",
       "7    description of a polynomial time reduction of ...   \n",
       "134  this paper considers the problem of reasoning ...   \n",
       "70   the invar package is introduced a fast manipul...   \n",
       "117  in this paper we survey the computational time...   \n",
       "12   this paper considers the use of punctured conv...   \n",
       "93   this paper deals with the computation of the r...   \n",
       "123  these are the notes for a set of lectures deli...   \n",
       "6    sparse code division multiple access a variati...   \n",
       "108  this paper deals with the problem of increasin...   \n",
       "180  when looking for a solution deterministic meth...   \n",
       "\n",
       "                                               authors  \\\n",
       "7                                         Sergey Gubin   \n",
       "134  Giorgio Terracina, Nicola Leone, Vincenzino Li...   \n",
       "70   Jose M. Martin-Garcia, Renato Portugal, Leon R...   \n",
       "117                                  Jonas Dieckelmann   \n",
       "12   Samuele Bandi, Luca Stabellini, Andrea Conti a...   \n",
       "93   Jean-Guillaume Dumas (LMC - IMAG), Philippe El...   \n",
       "123               Andrea Montanari and Rudiger Urbanke   \n",
       "6                             Jack Raymond, David Saad   \n",
       "108                                      Markus Grassl   \n",
       "180               Pierre Collet, Jean-Philippe Rennard   \n",
       "\n",
       "                                           journal-ref license  \n",
       "7                                                  NaN     NaN  \n",
       "134                                                NaN     NaN  \n",
       "70              Comp. Phys. Commun. 177 (2007) 640-648     NaN  \n",
       "117                                                NaN     NaN  \n",
       "12                                                 NaN     NaN  \n",
       "93                                                 NaN     NaN  \n",
       "123                                                NaN     NaN  \n",
       "6    J. Phys. A: Math. Theor. 40 No 41 (12 October ...     NaN  \n",
       "108  Proceedings 2007 IEEE International Symposium ...     NaN  \n",
       "180  Rennard, J.-P., Handbook of Research on Nature...     NaN  "
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers.iloc[sim_papers[0][:,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "97a4dcab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we describe a new algorithm the [eqn_latex]pebble game with colors and use it obtain a characterization of the family of [eqn_latex]sparse graphs and algorithmic solutions to a family of problems concerning tree decompositions of graphs special instances of sparse graphs appear in rigidity theory and have received increased attention in recent years in particular our colored pebbles generalize and strengthen the previous results of lee and streinu and give a new proof of the tuttenashwilliams characterization of arboricity we also present a new decomposition that certifies sparsity based on the [eqn_latex]pebble game with colors our work also exposes connections between pebble game algorithms and previous sparse graph algorithms by gabow gabow and westermann and hendrickson',\n",
       " 'in a quantum mechanical model diosi feldmann and kosloff arrived at a conjecture stating that the limit of the entropy of certain mixtures is the relative entropy as system size goes to infinity the conjecture is proven in this paper for density matrices the first proof is analytic and uses the quantum law of large numbers the second one clarifies the relation to channel capacity per unit cost for classicalquantum channels both proofs lead to generalization of the conjecture',\n",
       " 'the intelligent acoustic emission locator is described in part i while part ii discusses blind source separation time delay estimation and location of two simultaneously active continuous acoustic emission sources   the location of acoustic emission on complicated aircraft frame structures is a difficult problem of nondestructive testing this article describes an intelligent acoustic emission source locator the intelligent locator comprises a sensor antenna and a general regression neural network which solves the location problem based on learning from examples locator performance was tested on different test specimens tests have shown that the accuracy of location depends on sound velocity and attenuation in the specimen the dimensions of the tested area and the properties of stored data the location accuracy achieved by the intelligent locator is comparable to that obtained by the conventional triangulation method while the applicability of the intelligent locator is more general since analysis of sonic ray paths is avoided this is a promising method for nondestructive testing of aircraft frame structures by the acoustic emission method',\n",
       " 'part i describes an intelligent acoustic emission locator while part ii discusses blind source separation time delay estimation and location of two continuous acoustic emission sources   acoustic emission analysis is used for characterization and location of developing defects in materials acoustic emission sources often generate a mixture of various statistically independent signals a difficult problem of acoustic emission analysis is separation and characterization of signal components when the signals from various sources and the mode of mixing are unknown rec blind source separation bss by ndent component analysis ica has been used to solve these problems the purpose of this paper is to demonstrate the applicability of independent component analysis to locate two independent simultaneously active acoustic emission sources on an aluminum band specimen the method is promising for nondestructive testing of aircraft frame structures by acoustic emission analysis',\n",
       " 'in this paper we introduce the online viterbi algorithm for decoding hidden markov models in much smaller than linear space our analysis on twostate hidden markov models suggests that the expected maximum memory used to decode sequence of length [eqn_latex] with [eqn_latex]state hidden markov models can be as low as [eqn_latex] without a significant slowdown compared to the classical viterbi algorithm classical viterbi algorithm requires [eqn_latex] space which is impractical for analysis of long dna sequences such as complete human genome chromosomes and for continuous data streams we also experimentally demonstrate the performance of the online viterbi algorithm on a simple hidden markov models for gene finding on both simulated and real dna sequences',\n",
       " 'real options for project schedules has three recursive samplingoptimization shells an outer adaptive simulated annealing optimization shell optimizes parameters of strategic plans containing multiple projects containing ordered tasks a middle shell samples probability distributions of durations of tasks an inner shell samples probability distributions of costs of tasks pathtree is used to develop options on schedules algorithms used for trading in risk dimensions are applied to develop a relative risk analysis among projects',\n",
       " 'sparse code division multiple access a variation on the standard code division multiple access method in which the spreading signature matrix contains only a relatively small number of nonzero elements is presented and analysed using methods of statistical physics the analysis provides results on the performance of maximum likelihood decoding for sparse spreading codes in the large system limit we present results for both cases of regular and irregular spreading matrices for the binary additive whian noise channel biawgn with a comparison to the canonical dense random spreading code',\n",
       " 'description of a polynomial time reduction of sat to sat of polynomial size',\n",
       " 'this article has been withdrawn because it has been merged with the earlier article gct arxiv cs [cscc] in the series the merged article is now available as   geometric complexity theory iii on deciding nonvanishing of a littlewoodrichardson coefficient journal of algebraic combinatorics vol  issue   pp  authors ketan mulmuley hari narayanan and milind sohoni   the new article in this gct slot in the series is   geometric complexity theory v equivalence between blackbox derandomization of polynomial identity testing and derandomization of noethers normalization lemma in the proceedings of focs  abstract arxiv [cscc] full version author ketan mulmuley',\n",
       " 'given a multipleinput multipleoutput channel feedback from the receiver can be used to specify a transmit precoding matrix which selectively activates the strongest channel modes here we analyze the performance of random vector quantization in which the precoding matrix is selected from a random codebook containing independent isotropically distributed entries we assume that channel elements are iid and known to the receiver which relays the optimal ratemaximizing precoder codebook index to the transmitter using b bits we first derive the large system capacity of beamforming rankone precoding matrix as a function of b where large system refers to the limit as b and the number of transmit and receive antennas all go to infinity with fixed ratios with beamforming random vector quantization is asymptotically optimal ie no other quantization scheme can achieve a larger asymptotic rate the performance of random vector quantization is also compared with that of a simpler reducedrank scalar quantization scheme in which the beamformer is constrained to lie in a random subspace we subsequently consider a precoding matrix with arbitrary rank and approximate the asymptotic random vector quantization performance with optimainear receivers matched filter and minimum mean squared error mmse numerical examples show that these approximations accurately predict the performance of finitesize systems of interest given a target spectral efficiency numerical examples show that the amount of feedback required by the linear minimum mean squared error receiver is only slightly more than that required by the optimal receiver whereas the matched filter can require significantly more feedback',\n",
       " 'in some particular cases we give criteria for morphic sequences to be almost periodic uniformly recurrent namely we deal with fixed points of nonerasing morphisms and with automatic sequences in both cases a polynomialtime algorithm solving the problem is found a result more or less supporting the conjecture of decidability of the general problem is given',\n",
       " 'this article belongs to a series on geometric complexity theory an approach to the p vs np and related problems through algebraic geometry and representation theory the basic principle behind this approach is called the flip in essence it reduces the negative hypothesis in complexity theory the lower bound problems such as the p vs np problem in characteristic zero to the positive hypothesis in complexity theory the upper bound problems specifically to showing that the problems of deciding nonvanishing of the fundamental structural constants in representation theory and algebraic geometry such as the well known plethysm constantsor rather certain relaxed forms of these decision probelmsbelong to the complexity class p in this article we suggest a plan for implementing the flip ie for showing that these relaxed decision problems belong to p this is based on the reduction of the preceding complexitytheoretic positive hypotheses to mathematical positivity hypotheses specifically to showing that there exist positive formulaeie formulae with nonnegative coefficientsfor the structural constants under consideration and certain functions associated with them these turn out be intimately related to the similar positivity properties of the kazhdanlusztig polynomials and the multiplicative structural constants of the canonical global crystal bases in the theory of drinfeldjimbo quantum groups the known proofs of these positivity properties depend on the riemann hypothesis over finite fields and the related results thus the reduction here in conjunction with the flip in essence says that the validity of the p vs np conjecture in characteristic zero is intimately linked to the riemann hypothesis over finite fields and related problems',\n",
       " 'this paper considers the use of punctured convolutional codes to obtain pragmatic spacetime trellis codes over blockfading channel we show that good performance can be achieved even when puncturation is adopted and that we can still employ the same viterbi decoder of the convolutional mother code by using approximated metrics without increasing the complexity of the decoding operations',\n",
       " 'moore introduced a class of realvalued recursive functions by analogy with kleenes formulation of the standard recursive functions while his concise definition inspired a new line of research on analog computation it contains some technical inaccuracies focusing on his primitive recursive functions we pin down what is problematic and discuss possible attempts to remove the ambiguity regarding the behavior of the differential recursion operator on partial functions it turns out that in any case the purported relation to differentially algebraic functions and hence to shannons model of analog computation fails',\n",
       " 'this paper discusses the benefits of describing the world as information especially in the study of the evolution of life and cognition traditional studies encounter problems because it is difficult to describe life and cognition in terms of matter and energy since their laws are valid only at the physical scale however if matter and energy as well as life and cognition are described in terms of information evolution can be described consistently as information becoming more complex   the paper presents eight tentative laws of information valid at multiple scales which are generalizations of darwinian cybernetic thermodynamic psychological philosophical and complexity principles these are further used to discuss the notions of life cognition and their evolution',\n",
       " 'the hamiltonian cycle problem in digraphs d with degree bound two is solved by two mappings in this paper the first bijection is between an incidence matrix c_nm of simple digraph and an incidence matrix f of balanced bipartite undirected graph g the second mapping is from a perfect matching of g to a cycle of d it proves that the complexity of hamiltonian cycle problem in d is polynomial and finding a second nonisomorphism hamiltonian cycle from a given hamiltonian digraph with degree bound two is also polynomial lastly it deduces pnp base on the results',\n",
       " 'it has been observed that particular rate partially systematic parallel concatenated convolutional codes can achieve a lower error floor than that of their rate parent codes nevertheless good puncturing patterns can only be identified by means of an exhaustive search whilst convergence towards low bit error probabilities can be problematic when the systematic output of a rate partially systematic parallel concatenated convolutional codes is heavily punctured in this paper we present and study a family of rate partially systematic parallel concatenated convolutional codes which we call pseudorandomly punctured codes we evaluate their bit error rate performance and we show that they always yield a lower error floor than that of their rate parent codes furthermore we compare analytic results to simulations and we demonstrate that their performance converges towards the error floor region owning to the moderate puncturing of their systematic output consequently we propose pseudorandom puncturing as a means of improving the bandwidth efficiency of a parallel concatenated convolutional codes and simultaneously lowering its error floor',\n",
       " 'given a bipartite graph [eqn_latex] where edges take on both positive and negative weights from set [eqn_latex] the maximum weighted edge biclique problem or [eqn_latex]mweb for short asks to find a bipartite subgraph whose sum of edge weights is maximized this problem has various applications in bioinformatics machine learning and databases and its inapproximability remains open in this paper we show that for a wide range of choices of [eqn_latex] specifically when [eqn_latex] where [eqn_latex] and [eqn_latex] no polynomial time algorithm can approximate [eqn_latex]mweb within a factor of [eqn_latex] for some [eqn_latex] unless [eqn_latex] this hardness result gives justification of the heuristic approaches adopted for various applied problems in the aforementioned areas and indicates that good approximation algorithms are unlikely to exist specifically we give two applications by showing that  finding statistically significant biclusters in the samba model proposed in for the analysis of microarray data is [eqn_latex]inapproximable and  no polynomial time algorithm exists for the minimum description length with holes problem unless [eqn_latex]',\n",
       " 'we illustrate through example  and  that the condition at theorem  in [] dissatisfies necessity and the converse proposition of fact  in [] does not hold namely the condition zm  lak   ak is not sufficient for fi  fj  fk illuminate through an analysis and ex that there is a logic error during deduction of fact  which causes each of fact    to be invalid demonstrate through ex and  that each or the combination of qu  qu  d at fact  and table  at fact  is not sufficient for fi  fj  fk property      each are invalid and alg based on fact  and alg based on table  are disordered and wrong logically further manifest through a repeated experiment and ex that the data at table  is falsified and the example in [] is woven elaborately we explain why cx  ax  wfx  m is changed to cx  ax  wfxd  m in reesse v to the signature fraud we point out that [] misunderstands the existence of t and q  m and forging of q can be easily avoided through moving h therefore the conclusion of [] that reesse is not secure at all which connotes that [] can extract a related private key from any public key in reesse is fully incorrect and as long as the parameter omega is fitly selected reesse with cx  ax  wfx  m is secure',\n",
       " 'we investigate cooperative wireless relay networks in which the nodes can help each other in data transmission we study different coding strategies in the singlesource singledestination network with many relay nodes given the myriad of ways in which nodes can cooperate there is a natural routing problem ie determining an ordered set of nodes to relay the data from the source to the destination we find that for a given route the decodeandforward strategy which is an information theoretic cooperative coding strategy achieves rates significantly higher than that achievable by the usual multihop coding strategy which is a pointtopoint noncooperative coding strategy we construct an algorithm to find an optimal route in terms of rate maximizing for the decodeandforward strategy since the algorithm runs in factorial time in the worst case we propose a heuristic algorithm that runs in polynomial time the heuristic algorithm outputs an optimal route when the nodes transmit independent codewords we implement these coding strategies using practical low density parity check codes to compare the performance of the strategies on different routes',\n",
       " 'this paper investigates the manytoone throughput capacity and by symmetry onetomany throughput capacity of ieee  multihop networks it has generally been assumed in prior studies that the manytoone throughput capacity is upperbounded by the link capacity l throughput capacity l is not achievable under  this paper introduces the notion of canonical networks which is a class of regularlystructured networks whose capacities can be analyzed more easily than unstructured networks we show that the throughput capacity of canonical networks under  has an analytical upper bound of l when the source nodes are two or more hops away from the sink and simulated throughputs of l l when the source nodes are many hops away we conjecture that l is also the upper bound for general networks when all links have equal length l can be shown to be the upper bound for general networks our simulations show that  networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds fortunately by properly selecting routes near the gateway or by properly positioning the relay nodes leading to the gateway to fashion after the structure of canonical networks the throughput can be improved significantly by more than  indeed in a dense network it is worthwhile to deactivate some of the relay nodes near the sink judiciously',\n",
       " 'the interference channel with degraded message sets icdms refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium and one of the senders has complete and a priori noncausal knowledge about the message being transmitted by the other a coding scheme that collectively has advantages of cooperative coding collaborative coding and dirty paper coding is developed for such a channel with resorting to this coding scheme achievable rate regions of the icdms in both discrete memoryless and gaussian cases are derived which in general include several previously known rate regions numerical examples for the gaussian case demonstrate that in the highinterferencegain regime the derived achievable rate regions offer considerable improvements over these existing results',\n",
       " 'we present an algorithm for systematic encoding of hermitian codes for a hermitian code defined over [eqn_complexity] the proposed algorithm achieves a run time complexity of [eqn_complexity] and is suitable for vlsi implementation the encoder architecture uses as main blocks q varyingrate reedsolomon encoders and achieves a space complexity of [eqn_complexity] in terms of finite field multipliers and memory elements',\n",
       " 'the problem of statistical learning is to construct a predictor of a random variable [eqn_latex] as a function of a related random variable [eqn_latex] on the basis of an iid training sample from the joint distribution of [eqn_latex] allowable predictors are drawn from some specified class and the goal is to approach asymptotically the performance expected loss of the best predictor in the class we consider the setting in which one has perfect observation of the [eqn_latex]part of the sample while the [eqn_latex]part has to be communicated at some finite bit rate the encoding of the [eqn_latex]values is allowed to depend on the [eqn_latex]values under suitable regularity conditions on the admissible predictors the underlying family of probability distributions and the loss function we give an informationtheoretic characterization of achievable predictor performance in terms of conditional distortionrate functions the ideas are illustrated on the example of nonparametric regression in gaussian noise',\n",
       " 'the high volume of packets and packet rates of traffic on some router links makes it exceedingly difficult for routers to examine every packet in order to keep detailed statistics about the traffic which is traversing the router sampling is commonly applied on routers in order to limit the load incurred by the collection of information that the router has to undertake when evaluating flow information for monitoring purposes the sampling process in nearly all cases is a deterministic process of choosing  in every n packets on a perinterface basis and then forming the flow statistics based on the collected sampled statistics even though this sampling may not be significant for some statistics such as packet rate others can be severely distorted however it is important to consider the sampling techniques and their relative accuracy when applied to different traffic patterns the main disadvantage of sampling is the loss of accuracy in the collected trace when compared to the original traffic stream to date there has not been a detailed analysis of the impact of sampling at a router in various traffic profiles and flow criteria in this paper we assess the performance of the sampling process as used in netflow in detail and we discuss some techniques for the compensation of loss of monitoring detail',\n",
       " 'in this paper we give a definition of algorithm finite algorithm equivalent algorithms and what it means for a single algorithm to dominate a set of algorithms we define a derived algorithm which may have a smaller mean execution time than any of its component algorithms we give an explicit expression for the mean execution time when it exists of the derived algorithm we give several illustrative examples of derived algorithms with two component algorithms we include mean execution time solutions for twoalgorithm processors whose joint density of execution times are of several general forms for the case in which the joint density for a twoalgorithm processor is a step function we give a maximumlikelihood estimation scheme with which to analyze empirical processing time data',\n",
       " 'this paper develops a contentionbased opportunistic feedback technique towards relay selection in a dense wireless network this technique enables the forwarding of additional parity information from the selected relay to the destination for a given network the effects of varying key parameters such as the feedback probability are presented and discussed a primary advantage of the proposed technique is that relay selection can be performed in a distributed way simulation results find its performance to closely match that of centralized schemes that utilize gps information unlike the proposed method the proposed relay selection method is also found to achieve throughput gains over a pointtopoint transmission strategy',\n",
       " 'it has been shown that a decentralized relay selection protocol based on opportunistic feedback from the relays yields good throughput performance in dense wireless networks this selection strategy supports a hybridarq transmission approach where relays forward parity information to the destination in the event of a decoding error such an approach however suffers a loss compared to centralized strategies that select relays with the best channel gain to the destination this paper closes the performance gap by adding another level of channel feedback to the decentralized relay selection problem it is demonstrated that only one additional bit of feedback is necessary for good throughput performance the performance impact of varying key parameters such as the number of relays and the channel feedback threshold is discussed an accompanying bit error rate analysis demonstrates the importance of relay selection',\n",
       " 'we assess the practicality of random network coding by illuminating the issue of overhead and considering it in conjunction with increasingly long packets sent over the erasure channel we show that the transmission of increasingly long packets consisting of either of an increasing number of symbols per packet or an increasing symbol alphabet size results in a data rate approaching zero over the erasure channel this result is due to an erasure probability that increases with packet length numerical results for a particular modulation scheme demonstrate a data rate of approximately zero for a large but finitelength packet our results suggest a reduction in the performance gains offered by random network coding',\n",
       " 'a new incremental algorithm for data compression is presented for a sequence of input symbols algorithm incrementally constructs a padic integer number as an output decoding process starts with less significant part of a padic integer and incrementally reconstructs a sequence of input symbols algorithm is based on certain features of padic numbers and padic norm padic coding algorithm may be considered as of generalization a popular compression technique  arithmetic coding algorithms it is shown that for p   the algorithm works as integer variant of arithmetic coding for a special class of models it gives exactly the same codes as huffmans algorithm for another special model and a specific alphabet it gives golombrice codes',\n",
       " 'we study universal compression of sequences generated by monotonic distributions we show that for a monotonic distribution over an alphabet of size [eqn_latex] each probability parameter costs essentially [eqn_latex] bits where [eqn_latex] is the coded sequence length as long as [eqn_latex] otherwise for [eqn_latex] the total average sequence redundancy is [eqn_latex] bits overall we then show that there exists a subclass of monotonic distributions over infinite alphabets for which redundancy of [eqn_latex] bits overall is still achievable this class contains fast decaying distributions including many distributions over the integers and geometric distributions for some slower decays including other distributions over the integers redundancy of [eqn_latex] bits overall is achievable where a method to compute specific redundancy rates for such distributions is derived the results are specifically true for finite entropy monotonic distributions finally we study individual sequence redundancy behavior assuming a sequence is governed by a monotonic distribution we show that for sequences whose empirical distributions are monotonic individual redundancy bounds similar to those in the average case can be obtained however even if the monotonicity in the empirical distribution is violated diminishing per symbol individual sequence redundancies with respect to the monotonic maximum likelihood description length may still be achievable',\n",
       " 'this paper presents an experimental study and the lessons learned from the observation of the attackers when logged on a compromised machine the results are based on a six months period during which a controlled experiment has been run with a high interaction honeypot we correlate our findings with those obtained with a worldwide distributed system of lowinteraction honeypots',\n",
       " 'this paper presents a measurementbased availability assessment study using field data collected during a year period from  sunossolaris unix workstations and servers interconnected through a local area network we focus on the estimation of machine uptimes downtimes and availability based on the identification of failures that caused total service loss data corresponds to syslogd event logs that contain a large amount of information about the normal activity of the studied systems as well as their behavior in the presence of failures it is widely recognized that the information contained in such event logs might be incomplete or imperfect the solution investigated in this paper to address this problem is based on the use of auxiliary sources of data obtained from wtmpx files maintained by the sunossolaris unix operating system the results obtained suggest that the combined use of wtmpx and syslogd log files provides more complete information on the state of the target systems that is useful to provide availability estimations that better reflect reality',\n",
       " 'honeypots are more and more used to collect data on malicious activities on the internet and to better understand the strategies and techniques used by attackers to compromise target systems analysis and modeling methodologies are needed to support the characterization of attack processes based on the data collected from the honeypots this paper presents some empirical analyses based on the data collected from the leurrecom honeypot platforms deployed on the internet and presents some preliminary modeling studies aimed at fulfilling such objectives',\n",
       " 'for efficiency reasons the software system designers will is to use an integrated set of methods and tools to describe specifications and designs and also to perform analyses such as dependability schedulability and performance analysis and design language architecture analysis and design language has proved to be efficient for software architecture modeling in addition analysis and design language was designed to accommodate several types of analyses this paper presents an iterative dependencydriven approach for dependability modeling using analysis and design language it is illustrated on a small example this approach is part of a complete framework that allows the generation of dependability analysis and evaluation models from analysis and design language models to support the analysis of software and system architectures in critical application domains',\n",
       " 'we present a hierarchical simulation approach for the dependability analysis and evaluation of a highly available commercial cachebased raid storage system the architecture is complex and includes several layers of overlapping error detection and recovery mechanisms three abstraction levels have been developed to model the cache architecture cache operations and error detection and recovery mechanism the impact of faults and errors occurring in the cache and in the disks is analyzed at each level of the hierarchy a simulation submodel is associated with each abstraction level the models have been developed using depend a simulationbased environment for systemlevel dependability analysis which provides facilities to inject faults into a functional behavior model to simulate error detection and recovery mechanisms and to evaluate quantitative measures several fault models are defined for each submodel to simulate cache component failures disk failures transmission errors and data errors in the cache memory and in the disks some of the parameters characterizing fault injection in a given submodel correspond to probabilities evaluated from the simulation of the lowerlevel submodel based on the proposed methodology we evaluate and analyze  the system behavior under a real workload and high error rate focusing on error bursts  the coverage of the error detection mechanisms implemented in the system and the error latency distributions and  the accumulation of errors in the cache and in the disks',\n",
       " 'in a sensor network in practice the communication among sensors is subject to errors or failures at random times  costs and constraints since sensors and networks operate under scarce resources such as power data rate or communication the signaltonoise ratio is usually a main factor in determining the probability of error or of communication failure in a link these probabilities are then a proxy for the signaltonoise ratio under which the links operate the paper studies the problem of designing the topology ie assigning the probabilities of reliable communication among sensors or of link failures to maximize the rate of convergence of average consensus when the link communication costs are taken into account and there is an overall communication budget constraint to consider this problem we address a number of preliminary issues  model the network as a random topology  establish necessary and sufficient conditions forsquare sense mss and almost sure as convergence of average consensus when network links fail and in particular  show that a necessary and sufficient condition for both mean square sense and as convergence is for the algebraic connectivity of the mean graph describing the network topology to be strictly positive with these results we formulate topology design subject to random link failures and to a communication cost constraint as a constrained convex optimization problem to which we apply semidefinite programming techniques we show by an extensive numerical study that the optimal design improves significantly the convergence speed of the consensus algorithm and can achieve the asymptotic performance of a nonrandom network at a fraction of the communication cost',\n",
       " 'mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks in a mimobased mesh network the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel recently researchers showed that ``dirty paper coding is the optimal transmission strategy for gaussian vector broadcast channels so far there has been little study on how this fundamental result will impact the crosslayer design for mimobased mesh networks to fill this gap we consider the problem of jointly optimizing dirty paper coding power allocation in the link layer at each node and multihopmultipath routing in a mimobased mesh networks it turns out that this optimization problem is a very challenging nonconvex problem to address this difficulty we transform the original problem to an equivalent problem by exploiting the channel duality for the transformed problem we develop an efficient solution procedure that integrates lagrangian dual decomposition method conjugate gradient projection method based on matrix differential calculus cuttingplane method and subgradient method in our numerical example it is shown that we can achieve a network performance gain of  by using dirty paper coding',\n",
       " 'advances in semiconductor technology are contributing to the increasing complexity in the design of embedded systems architectures with novel techniques such as evolvable nature and autonomous behavior have engrossed lot of attention this paper demonstrates conceptually evolvable embedded systems can be characterized basing on acausal nature it is noted that in acausal systems future input needs to be known here we make a mechanism such that the system predicts the future inputs and exhibits pseudo acausal nature an embedded system that uses theoretical framework of acausality is proposed our method aims at a novel architecture that features the hardware evolability and autonomous behavior alongside pseudo acausality various aspects of this architecture are discussed in detail along with the limitations',\n",
       " 'the online shortest path problem is considered under various models of partial monitoring given a weighted directed acyclic graph whose edge weights can change in an arbitrary adversarial way a decision maker has to choose in each round of a game a path between two distinguished vertices such that the loss of the chosen path defined as the sum of the weights of its composing edges be as small as possible in a setting generalizing the multiarmed bandit problem after choosing a path the decision maker learns only the weights of those edges that belong to the chosen path for this problem an algorithm is given whose average cumulative loss in n rounds exceeds that of the best path matched offline to the entire sequence of the edge weights by a quantity that is proportional to n and depends only polynomially on the number of edges of the graph the algorithm can be implemented with linear complexity in the number of rounds n and in the number of edges an extension to the socalled label efficient setting is also given in which the decision maker is informed about the weights of the edges corresponding to the chosen path at a total of m  n time instances another extension is shown where the decision maker competes against a timevarying path a generalization of the problem of tracking the best expert a version of the multiarmed bandit setting for shortest path is also discussed where the decision maker learns only the total weight of the chosen path but not the weights of the individual edges on the path applications to routing in packet switched networks along with simulation results are also presented',\n",
       " 'ordinal regression is an important type of learning which has properties of both classification and regression here we describe a simple and effective approach to adapt a traditional neural network to learn ordinal categories our approach is a generalization of the perceptron method for ordinal regression on several benchmark datasets our method nnrank outperforms a neural network classification method compared with the ordinal regression methods using gaussian processes and support vector machines nnrank achieves comparable performance moreover nnrank has the advantages of traditional neural networks learning in both online and batch modes handling very large training datasets and making rapid predictions these features make nnrank a useful and complementary tool for largescale data processing tasks such as information retrieval web page ranking collaborative filtering and protein ranking in bioinformatics',\n",
       " 'a drawback of kolmogorovchaitin complexity k as a function from s to the shortest program producing s is its noncomputability which limits its range of applicability moreover when strings are short the dependence of k on a particular universal turing machine u can be arbitrary in practice one can approximate it by computable compression methods however such compression methods do not always provide meaningful approximationsfor strings shorter for example than typical compiler lengths in this paper we suggest an empirical approach to overcome this difficulty and to obtain a stable definition of the kolmogorovchaitin complexity for short sequences additionally a correlation in terms of distribution frequencies was found across the output of two models of abstract machines namely unidimensional cellular automata and deterministic turing machine',\n",
       " 'efficiently computing fast paths in large scale dynamic road networks where dynamic traffic information is known over a part of the network is a practical problem faced by several traffic information service providers who wish to offer a realistic fast path computation to gps terminal enabled vehicles the heuristic solution method we propose is based on a highway hierarchybased shortest path algorithm for static largescale networks we maintain a static highway hierarchy and perform each query on the dynamically evaluated network',\n",
       " 'this paper is concerned with optimum diversity receiver structure and its performance analysis of differential phase shift keying with differential detection over nonselective independent nonidentically distributed rayleigh fading channels the fading process in each branch is assumed to have an arbitrary doppler spectrum with arbitrary doppler bandwidth but to have distinct asymmetric fading power spectral density characteristic using differential phase shift keying as an example the avbit error probability bep of the optimum diversity receiver is obtained by calculating the bit error probability for each of the three individual bits the bit error probability results derived are given in exact explicit closedform expressions which show clearly the behavior of the performance as a function of various system parameters',\n",
       " 'the subject of collective attention is central to an information age where millions of people are inundated with daily messages it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations we have analyzed the dynamics of collective attention among one million users of an interactive website  diggcom  devoted to thousands of novel news stories the observations can be described by a dynamical model characterized by a single novelty factor our measurements indicate that novelty within groups decays with a stretchedexponential law suggesting the existence of a natural time scale over which attention fades',\n",
       " 'hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multiobjective optimizers the best known algorithm to calculate it for [eqn_latex] points in [eqn_latex]dimensional space has a run time of [eqn_latex] with special data structures this paper presents a recursive vertexsplitting algorithm for calculating the hypervolume indicator of a set of [eqn_latex] noncomparable points in [eqn_latex] dimensions it splits out multiple child hypercuboids which can not be dominated by a splitting reference point in special the splitting reference point is carefully chosen to minimize the number of points in the child hypercuboids the complexity analysis shows that the proposed algorithm achieves [eqn_latex] time and [eqn_latex] space complexity in the worst case',\n",
       " 'we present a genetic algorithm which is distributed in two novel ways along genotype and temporal axes our algorithm first distributes for every member of the population a subset of the genotype to each network node rather than a subset of the population to each this genotype distribution is shown to offer a significant gain in running time then for efficient use of the computational resources in the network our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain rather that in the spatial domain this temporal distribution may lead to temporal inconsistency in selection and replacement however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties',\n",
       " 'there is a huge amount of historical documents in libraries and in various national archives that have not been exploited electronically although automatic reading of complete pages remains in most cases a longterm objective tasks such as word spotting textimage alignment authentication and extraction of specific fields are in use today for all these tasks a major step is document segmentation into text lines because of the low quality and the complexity of these documents background noise artifacts due to aging interfering linesautomatic text line segmentation remains an open research field the objective of this paper is to present a survey of existing methods developed during the last decade and dedicated to documents of historical interest',\n",
       " 'we consider the problem of coloring the vertices of a large sparse random graph with a given number of colors so that no adjacent vertices have the same color using the cavity method we present a detailed and systematic analytical study of the space of proper colorings solutions   we show that for a fixed number of colors and as the average vertex degree number of constraints increases the set of solutions undergoes several phase transitions similar to those observed in the mean field theory of glasses first at the clustering transition the entropically dominant part of the phase space decomposes into an exponential number of pure states so that beyond this transition a uniform sampling of solutions becomes hard afterward the space of solutions condenses over a finite number of the largest states and consequently the total entropy of solutions becomes smaller than the annealed one another transition takes place when in all the entropically dominant states a finite fraction of nodes freezes so that each of these nodes is allowed a single color in all the solutions inside the state eventually above the coloring threshold no more solutions are available we compute all the critical connectivities for erdosrenyi and regular random graphs and determine their asymptotic values for large number of colors   finally we discuss the algorithmic consequences of our findings we argue that the onset of computational hardness is not associated with the clustering transition and we suggest instead that the freezing transition might be the relevant phenomenon we also discuss the performance of a simple local walkcol algorithm and of the belief propagation algorithm in the light of our results',\n",
       " 'this paper uncovers and explores the close relationship between monte carlo optimization of a parametrized integral mco parametric machinelearning and `blackbox or `oraclebased optimization we make four contributions first we prove that mco is mathematically identical to a broad class of parametric machinelearning problems this identity potentially provides a new application domain for all broadly applicable parametric machinelearning techniques mco second we introduce immediate samp a new version of the probability collectives pc algorithm for blackbox optimization immediate sampling transforms the original based optimization problem into an mco problem accordingly by combining these first two contributions we can apply all parametric machinelearning techniques to based optimization in our third contribution we validate this way of improving based optimization by demonstrating that crossvalidation and bagging improve immediate sampling finally conventional mc and mco procedures ignore the relationship between the sample point locations and the associated values of the integrand only the values of the integrand at those locations are considered we demonstrate that one can exploit the sample location information using parametric machinelearning techniques for example by forming a fit of the sample locations to the associated values of the integrand this provides an additional way to apply parametric machinelearning techniques to improve mco',\n",
       " 'many organizations aspire to adopt agile processes to take advantage of the numerous benefits that it offers to an organization those benefits include but are not limited to quicker return on investment better software quality and higher customer satisfaction to date however there is no structured process at least in the public domain that guides organizations in adopting agile practices to address this problem we present the agile adoption framework the framework consists of two components an agile measurement index and a stage process that together guide and assist the agile adoption efforts of organizations more specifically the agile measurement index is used to identify the agile potential of projects and organizations the stage process on the other hand helps determine a whether or not organizations are ready for agile adoption and b guided by their potential what set of agile practices can and should be introduced',\n",
       " 'a multiple antenna downlink channel where limited channel feedback is available to the transmitter is considered in a vector downlink channel single antenna at each receiver the transmit antenna array can be used to transmit separate data streams to multiple receivers only if the transmitter has very accurate channel knowledge ie if there is highrate channel feedback from each receiver in this work it is shown that channel feedback requirements can be significantly reduced if each receiver has a small number of antennas and appropriately combines its antenna outputs a combining method that minimizes channel quantization error at each receiver and thereby minimizes multiuser interference is proposed and analyzed this technique is shown to outperform traditional techniques such as maximumratio combining because minimization of interference power is more critical than maximization of signal power in the multiple antenna downlink analysis is provided to quantify the feedback savings and the technique is seen to work well with user selection and is also robust to receiver estimation error',\n",
       " 'low density lattice codes are novel lattice codes that can be decoded efficiently and approach the capacity of the additive white gaussian noise channel in low density lattice codes a codeword x is generated directly at the ndimensional euclidean space as a linear transformation of a corresponding integer message vector b ie x  gb where h the inverse of g is restricted to be sparse the fact that h is sparse is utilized to develop a lineartime iterative decoding scheme which attains as demonstrated by simulations good error performance within ~db from capacity at block length of n   symbols the paper also discusses convergence results and implementation considerations',\n",
       " 'this paper reports on work aimed at supporting knowledge and expertise finding within a large research and development organisation the paper first discusses the nature of knowledge important to research and development organisations and presents a prototype information system developed to support knowledge and expertise finding the paper then discusses a trial of the system within an research and development organisation the implications and limitations of the trial and discusses future research questions',\n",
       " 'distancepreserving mappings are mappings from the set of all qary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors in this paper we propose a construction of distancepreserving mappings from ternary vectors the constructed distancepreserving mappings improve the lower bounds on the maximal size of permutation arrays',\n",
       " 'the secure and robust functioning of a network relies on the defectfree implementation of network applications as network protocols have become increasingly complex however handwriting network message processing code has become increasingly errorprone in this paper we present a domainspecific language zebu for describing protocol message formats and related processing constraints from a zebu specification a compiler automatically generates stubs to be used by an application to parse network messages zebu is easy to use as it builds on notations used in rfcs to describe protocol grammars zebu is also efficient as the memory usage is tailored to application needs and message fragments can be specified to be processed on demand finally zebubased applications are robust as the zebu compiler automatically checks specification consistency and generates parsing stubs that include validation of the message structure using a mutation analysis in the context of sip and rtsp we show that zebu significantly improves application robustness',\n",
       " 'in these notes we formally describe the functionality of calculating valid domains from the bdd representing the solution space of valid configurations the formalization is largely based on the clab configuration framework',\n",
       " 'this paper has been withdrawn by the author this draft is withdrawn for its poor quality in english unfortunately produced by the author when he was just starting his science route look at the icml version instead httpicmlcshelsinkifipaperspdf',\n",
       " 'most design approaches for trelliscoded quantization take advantage of the duality of trelliscoded quantization with trelliscoded modulation and use the same empiricallyfound convolutional codes to label the trellis branches this letter presents an alternative approach that instead takes advantage of maximumhammingdistance convolutional codes the proposed source codes are shown to be competitive with the best in the literature for the same computational complexity',\n",
       " 'we consider the problem of estimating the probability of an observed string drawn iid from an unknown distribution the key feature of our study is that the length of the observed string is assumed to be of the same order as the size of the underlying alphabet in this setting many letters are unseen and the empirical distribution tends to overestimate the probability of the observed letters to overcome this problem the traditional approach to probability estimation is to use the classical goodturing estimator we introduce a natural scaling model and use it to show that the goodturing sequence probability estimator is not consistent we then introduce a novel sequence probability estimator that is indeed consistent under the natural scaling model',\n",
       " 'this paper presents new lowcomplexity latticedecoding algorithms for noncoherent block detection of qam and pam signals over complexvalued fading channels the algorithms are optimal in terms of the generalized likelihood ratio test the computational complexity is polynomial in the block length making generalized likelihood ratio testoptimal noncoherent detection feasible for implementation we also provide even lower complexity suboptimal algorithms simulations show that the suboptimal algorithms have performance indistinguishable from the optimal algorithms finally we consider block based transmission and propose to use noncoherent detection as an alternative tt assisted transmission pat the new technique is shown to outperform pilot assisted transmission',\n",
       " 'the class of interval graphs has been introduced for modelling scheduling and allocation problems and more recently for specific bioinformatic problems some of those applications imply restrictions on the interval graphs and justify the introduction of a hierarchy of subclasses of interval graphs that generalize line graphs balanced interval graphs unit interval graphs and xxinterval graphs we provide instances that show that all the inclusions are strict we extend the npcompleteness proof of recognizing interval graphs to the recognition of balanced interval graphs finally we give hints on the complexity of unit interval graphs recognition by studying relationships with other graph classes proper circulararc quasiline graphs k_free graphs ',\n",
       " 'information integration applications such as mediators or mashups that require access to information resources currently rely on users manually discovering and integrating them in the application manual resource discovery is a slow process requiring the user to sift through results obtained via keywordbased search although search methods have advanced to include evidence from document contents its metadata and the contents and link structure of the referring pages they still do not adequately cover information sources  often called ``the hidden web that dynamically generate documents in response to a query the recently popular social bookmarking sites which allow users to annotate and share metadata about various information sources provide rich evidence for resource discovery in this paper we describe a probabilistic model of the user annotation process in a social bookmarking system delicious we then use the model to automatically find resources relevant to a particular information domain our experimental results on data obtained from delicious show this approach as a promising method for helping automate the resource discovery task',\n",
       " 'the social media site flickr allows users to upload their photos annotate them with tags submit them to groups and also to form social networks by adding other users as contacts flickr offers multiple ways of browsing or searching it one option is tag search which returns all images tagged with a specific keyword if the keyword is ambiguous eg ``beetle could mean an insect or a car tag search results will include many images that are not relevant to the sense the user had in mind when executing the query we claim that users express their photography interests through the metadata they add in the form of contacts and image annotations we show how to exploit this metadata to personalize search results for the user thereby improving search performance first we show that we can significantly improve search precision by filtering tag search results by users contacts or a larger social network that includes those contacts contacts secondly we describe a probabilistic model that takes advantage of tag information to discover latent topics contained in the search results the users interests can similarly be described by the tags they used for annotating their images the latent topics found by the model are then used to personalize search results by finding images on topics that are of interest to the user']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts[0:64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "57f98c81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 317])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(abstracts[0:64], return_tensors='pt', padding=True)['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1e7463c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(abstracts[9449 * 64:9450*64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d80fe7d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ceil(3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0cdbf410",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m bert_tokens \u001b[38;5;241m=\u001b[39m tokenizer(papers\u001b[38;5;241m.\u001b[39mabstract\u001b[38;5;241m.\u001b[39mtolist(), return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Stefa\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2872\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2870\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   2871\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 2872\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_one(text\u001b[38;5;241m=\u001b[39mtext, text_pair\u001b[38;5;241m=\u001b[39mtext_pair, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mall_kwargs)\n\u001b[0;32m   2873\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2874\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32mc:\\Users\\Stefa\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2958\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2953\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2954\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match batch length of `text_pair`:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2955\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2956\u001b[0m         )\n\u001b[0;32m   2957\u001b[0m     batch_text_or_text_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[1;32m-> 2958\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[0;32m   2959\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   2960\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2961\u001b[0m         padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m   2962\u001b[0m         truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   2963\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m   2964\u001b[0m         stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[0;32m   2965\u001b[0m         is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[0;32m   2966\u001b[0m         pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2967\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2968\u001b[0m         return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   2969\u001b[0m         return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   2970\u001b[0m         return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   2971\u001b[0m         return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   2972\u001b[0m         return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   2973\u001b[0m         return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m   2974\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m   2975\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2976\u001b[0m     )\n\u001b[0;32m   2977\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2978\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[0;32m   2979\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[0;32m   2980\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2996\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2997\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Stefa\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3149\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   3139\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   3140\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   3141\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m   3142\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3146\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3147\u001b[0m )\n\u001b[1;32m-> 3149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_encode_plus(\n\u001b[0;32m   3150\u001b[0m     batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   3151\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   3152\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[0;32m   3153\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   3154\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m   3155\u001b[0m     stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[0;32m   3156\u001b[0m     is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[0;32m   3157\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   3158\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m   3159\u001b[0m     return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   3160\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   3161\u001b[0m     return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   3162\u001b[0m     return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   3163\u001b[0m     return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   3164\u001b[0m     return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m   3165\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m   3166\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3167\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Stefa\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\transformers\\tokenization_utils.py:803\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    801\u001b[0m     ids, pair_ids \u001b[38;5;241m=\u001b[39m ids_or_pair_ids\n\u001b[1;32m--> 803\u001b[0m first_ids \u001b[38;5;241m=\u001b[39m get_input_ids(ids)\n\u001b[0;32m    804\u001b[0m second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(pair_ids) \u001b[38;5;28;01mif\u001b[39;00m pair_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    805\u001b[0m input_ids\u001b[38;5;241m.\u001b[39mappend((first_ids, second_ids))\n",
      "File \u001b[1;32mc:\\Users\\Stefa\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\transformers\\tokenization_utils.py:770\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus.<locals>.get_input_ids\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_input_ids\u001b[39m(text):\n\u001b[0;32m    769\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 770\u001b[0m         tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenize(text, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    771\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(tokens)\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Stefa\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\transformers\\tokenization_utils.py:617\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.tokenize\u001b[1;34m(self, text, **kwargs)\u001b[0m\n\u001b[0;32m    615\u001b[0m         tokenized_text\u001b[38;5;241m.\u001b[39mappend(token)\n\u001b[0;32m    616\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 617\u001b[0m         tokenized_text\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenize(token))\n\u001b[0;32m    618\u001b[0m \u001b[38;5;66;03m# [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\u001b[39;00m\n\u001b[0;32m    619\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_text\n",
      "File \u001b[1;32mc:\\Users\\Stefa\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\transformers\\models\\bert\\tokenization_bert.py:245\u001b[0m, in \u001b[0;36mBertTokenizer._tokenize\u001b[1;34m(self, text, split_special_tokens)\u001b[0m\n\u001b[0;32m    243\u001b[0m split_tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_basic_tokenize:\n\u001b[1;32m--> 245\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbasic_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(\n\u001b[0;32m    246\u001b[0m         text, never_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_special_tokens \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m split_special_tokens \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    247\u001b[0m     ):\n\u001b[0;32m    248\u001b[0m         \u001b[38;5;66;03m# If the token is part of the never_split set\u001b[39;00m\n\u001b[0;32m    249\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbasic_tokenizer\u001b[38;5;241m.\u001b[39mnever_split:\n\u001b[0;32m    250\u001b[0m             split_tokens\u001b[38;5;241m.\u001b[39mappend(token)\n",
      "File \u001b[1;32mc:\\Users\\Stefa\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\transformers\\models\\bert\\tokenization_bert.py:423\u001b[0m, in \u001b[0;36mBasicTokenizer.tokenize\u001b[1;34m(self, text, never_split)\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;66;03m# union() returns a new set by concatenating the two sets.\u001b[39;00m\n\u001b[0;32m    422\u001b[0m never_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnever_split\u001b[38;5;241m.\u001b[39munion(\u001b[38;5;28mset\u001b[39m(never_split)) \u001b[38;5;28;01mif\u001b[39;00m never_split \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnever_split\n\u001b[1;32m--> 423\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clean_text(text)\n\u001b[0;32m    425\u001b[0m \u001b[38;5;66;03m# This was added on November 1st, 2018 for the multilingual and Chinese\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;66;03m# models. This is also applied to the English models now, but it doesn't\u001b[39;00m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;66;03m# matter since the English models were not trained on any Chinese data\u001b[39;00m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;66;03m# and generally don't have any Chinese data in them (there are Chinese\u001b[39;00m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;66;03m# characters in the vocabulary because Wikipedia does have some Chinese\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;66;03m# words in the English Wikipedia.).\u001b[39;00m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenize_chinese_chars:\n",
      "File \u001b[1;32mc:\\Users\\Stefa\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\transformers\\models\\bert\\tokenization_bert.py:525\u001b[0m, in \u001b[0;36mBasicTokenizer._clean_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m text:\n\u001b[0;32m    524\u001b[0m     cp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mord\u001b[39m(char)\n\u001b[1;32m--> 525\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cp \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m cp \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0xFFFD\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m _is_control(char):\n\u001b[0;32m    526\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    527\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_whitespace(char):\n",
      "File \u001b[1;32mc:\\Users\\Stefa\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\transformers\\tokenization_utils.py:289\u001b[0m, in \u001b[0;36m_is_control\u001b[1;34m(char)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m char \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m char \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m char \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m cat \u001b[38;5;241m=\u001b[39m unicodedata\u001b[38;5;241m.\u001b[39mcategory(char)\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cat\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bert_tokens = tokenizer(papers.abstract.tolist(), return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e3d1d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d14e977",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac9f2703",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m svd \u001b[38;5;241m=\u001b[39m TruncatedSVD(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m reduced_vec \u001b[38;5;241m=\u001b[39m svd\u001b[38;5;241m.\u001b[39mfit_transform(abstract_vec)\n",
      "File \u001b[1;32mc:\\Users\\Stefa\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Stefa\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Stefa\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:246\u001b[0m, in \u001b[0;36mTruncatedSVD.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components \u001b[38;5;241m>\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m    242\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    243\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_components(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) must be <=\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    244\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m n_features(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    245\u001b[0m         )\n\u001b[1;32m--> 246\u001b[0m     U, Sigma, VT \u001b[38;5;241m=\u001b[39m randomized_svd(\n\u001b[0;32m    247\u001b[0m         X,\n\u001b[0;32m    248\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components,\n\u001b[0;32m    249\u001b[0m         n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter,\n\u001b[0;32m    250\u001b[0m         n_oversamples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_oversamples,\n\u001b[0;32m    251\u001b[0m         power_iteration_normalizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpower_iteration_normalizer,\n\u001b[0;32m    252\u001b[0m         random_state\u001b[38;5;241m=\u001b[39mrandom_state,\n\u001b[0;32m    253\u001b[0m     )\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomponents_ \u001b[38;5;241m=\u001b[39m VT\n\u001b[0;32m    257\u001b[0m \u001b[38;5;66;03m# As a result of the SVD approximation error on X ~ U @ Sigma @ V.T,\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;66;03m# X @ V is not the same as U @ Sigma\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Stefa\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\sklearn\\utils\\extmath.py:450\u001b[0m, in \u001b[0;36mrandomized_svd\u001b[1;34m(M, n_components, n_oversamples, n_iter, power_iteration_normalizer, transpose, flip_sign, random_state, svd_lapack_driver)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transpose:\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;66;03m# this implementation is a bit faster with smaller shape[1]\u001b[39;00m\n\u001b[0;32m    448\u001b[0m     M \u001b[38;5;241m=\u001b[39m M\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m--> 450\u001b[0m Q \u001b[38;5;241m=\u001b[39m randomized_range_finder(\n\u001b[0;32m    451\u001b[0m     M,\n\u001b[0;32m    452\u001b[0m     size\u001b[38;5;241m=\u001b[39mn_random,\n\u001b[0;32m    453\u001b[0m     n_iter\u001b[38;5;241m=\u001b[39mn_iter,\n\u001b[0;32m    454\u001b[0m     power_iteration_normalizer\u001b[38;5;241m=\u001b[39mpower_iteration_normalizer,\n\u001b[0;32m    455\u001b[0m     random_state\u001b[38;5;241m=\u001b[39mrandom_state,\n\u001b[0;32m    456\u001b[0m )\n\u001b[0;32m    458\u001b[0m \u001b[38;5;66;03m# project M to the (k + p) dimensional space using the basis vectors\u001b[39;00m\n\u001b[0;32m    459\u001b[0m B \u001b[38;5;241m=\u001b[39m safe_sparse_dot(Q\u001b[38;5;241m.\u001b[39mT, M)\n",
      "File \u001b[1;32mc:\\Users\\Stefa\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\sklearn\\utils\\extmath.py:278\u001b[0m, in \u001b[0;36mrandomized_range_finder\u001b[1;34m(A, size, n_iter, power_iteration_normalizer, random_state)\u001b[0m\n\u001b[0;32m    276\u001b[0m     Q \u001b[38;5;241m=\u001b[39m safe_sparse_dot(A\u001b[38;5;241m.\u001b[39mT, Q)\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m power_iteration_normalizer \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLU\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 278\u001b[0m     Q, _ \u001b[38;5;241m=\u001b[39m linalg\u001b[38;5;241m.\u001b[39mlu(safe_sparse_dot(A, Q), permute_l\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    279\u001b[0m     Q, _ \u001b[38;5;241m=\u001b[39m linalg\u001b[38;5;241m.\u001b[39mlu(safe_sparse_dot(A\u001b[38;5;241m.\u001b[39mT, Q), permute_l\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m power_iteration_normalizer \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQR\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Stefa\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\sklearn\\utils\\extmath.py:193\u001b[0m, in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    191\u001b[0m         ret \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(a, b)\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 193\u001b[0m     ret \u001b[38;5;241m=\u001b[39m a \u001b[38;5;241m@\u001b[39m b\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    196\u001b[0m     sparse\u001b[38;5;241m.\u001b[39missparse(a)\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(b)\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m dense_output\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(ret, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoarray\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    200\u001b[0m ):\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39mtoarray()\n",
      "File \u001b[1;32mc:\\Users\\Stefa\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\scipy\\sparse\\_base.py:678\u001b[0m, in \u001b[0;36m_spbase.__matmul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m isscalarlike(other):\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScalar operands are not allowed, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    677\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m instead\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mul_dispatch(other)\n",
      "File \u001b[1;32mc:\\Users\\Stefa\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\scipy\\sparse\\_base.py:580\u001b[0m, in \u001b[0;36m_spbase._mul_dispatch\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mul_vector(other\u001b[38;5;241m.\u001b[39mravel())\u001b[38;5;241m.\u001b[39mreshape(M, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    579\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m other\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m other\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m N:\n\u001b[1;32m--> 580\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mul_multivector(other)\n\u001b[0;32m    582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m isscalarlike(other):\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;66;03m# scalar value\u001b[39;00m\n\u001b[0;32m    584\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mul_scalar(other)\n",
      "File \u001b[1;32mc:\\Users\\Stefa\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:507\u001b[0m, in \u001b[0;36m_cs_matrix._mul_multivector\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;66;03m# csr_matvecs or csc_matvecs\u001b[39;00m\n\u001b[0;32m    506\u001b[0m fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_sparsetools, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_matvecs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 507\u001b[0m fn(M, N, n_vecs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindptr, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata,\n\u001b[0;32m    508\u001b[0m    other\u001b[38;5;241m.\u001b[39mravel(), result\u001b[38;5;241m.\u001b[39mravel())\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "svd = TruncatedSVD(n_components=1024)\n",
    "reduced_vec = svd.fit_transform(abstract_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1174f9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86edd55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(600_000 // 50 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd94079",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.fit_transform(reduced_vec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
