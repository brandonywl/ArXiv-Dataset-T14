{"cells":[{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":291,"status":"ok","timestamp":1710082419485,"user":{"displayName":"Bernard Cheng","userId":"09763001268899605911"},"user_tz":-480},"id":"TFUSdmlkBSJl"},"outputs":[],"source":["############################################## begin replaying the code ##################################################\n","# https://www.kaggle.com/code/steubk/arxiv-taxonomy-e-top-influential-papers/notebook\n","\n","# we will do the same as the above kaggle project, but need to make some change to accommodate some missing files or big files\n"]},{"cell_type":"markdown","metadata":{"id":"WQ1wvHkTBSJm"},"source":["# Notes\n","\n","## **Purpose:**\n","- separate the data arxiv-metadata-oai-snapshot.json into multiple files, categorized by year, and extract information from arxiv-metadata-oai-snapshot.json and save to different .csv files\n","- extract information from internal-references-pdftotext.json and save it to a .csv file\n","\n","## **Data File:**\n","1. The metadata comes from https://www.kaggle.com/datasets/Cornell-University/arxiv. It contains the latest data till now.\n","\n","2. Internal reference data comes from https://github.com/mattbierbaum/arxiv-public-datasets/releases. The file is https://github.com/mattbierbaum/arxiv-public-datasets/releases/download/v0.2.0/internal-references-v0.2.0-2019-03-01.json.gz\n","\n","\n","*---------Please note-----------*\n","\n","The reference code from project \"arXiv - Taxonomy e Top Influential Papers\" ( https://www.kaggle.com/code/steubk/arxiv-taxonomy-e-top-influential-papers/input?select=arxiv-metadata-oai-snapshot-2020-08-14.json) is using the same dataset, but the dataset version is earlier \"arxiv-metadata-oai-snapshot-2020-08-14.json\", so the generated result is different because the up-to-date dataset contains data beyond 2020.*\n","\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3008,"status":"ok","timestamp":1710082422819,"user":{"displayName":"Bernard Cheng","userId":"09763001268899605911"},"user_tz":-480},"id":"PVfMX7cxtnrz","outputId":"f46b3305-41e3-4562-f617-ae8c4e5a110f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"mgAz46oNBSJn"},"source":["# **How to Run:**\n","- There are two parts in this notebook\n","- First part is used to extract info from the two huge json files. All the generated .csv files will be put into subfolder \"data\".\n","    - Please run the cells in sequence.\n","    - **When the code requires to upload Kaggle profile, upload your Kaggle profile.(cell 3)** This profile will be stored in colab session temporarily, so it will not get shared with others once you exit your colab session.\n","    - **Please mount your google drive by running the corresponding cell below (cell 4)**. Otherwise, the generated data file could not get saved.\n","    - **please specify the variable project_path(cell 5)** to point to your google drive data location in colab. Other variables such as project_data_path, metadata_file, ref_file is specified already, and you do not need to change them.\n","\n","- second part is used to do data analysis.\n","- part 1 and part 2 are independent. That means, once you finish running part 1, you could shut down your notebook. The next time it is opened, you could start running from part 2.\n","\n","- Please refer to https://www.kaggle.com/code/steubk/arxiv-taxonomy-e-top-influential-papers/notebook when necessary. Pls note that some steps are updated to accomodate the limited momery and CPU resource."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20612,"status":"ok","timestamp":1710082443428,"user":{"displayName":"Bernard Cheng","userId":"09763001268899605911"},"user_tz":-480},"id":"sQAl_vSl_Hit","outputId":"502c8b24-8759-45a9-bfcb-3c0917ed79e0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.6)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.2.2)\n","Requirement already satisfied: wordcloud in /usr/local/lib/python3.10/dist-packages (1.9.3)\n","Requirement already satisfied: numpy\u003e=1.6.1 in /usr/local/lib/python3.10/dist-packages (from wordcloud) (1.25.2)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from wordcloud) (9.4.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from wordcloud) (3.7.1)\n","Requirement already satisfied: contourpy\u003e=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib-\u003ewordcloud) (1.2.0)\n","Requirement already satisfied: cycler\u003e=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib-\u003ewordcloud) (0.12.1)\n","Requirement already satisfied: fonttools\u003e=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib-\u003ewordcloud) (4.49.0)\n","Requirement already satisfied: kiwisolver\u003e=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib-\u003ewordcloud) (1.4.5)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib-\u003ewordcloud) (23.2)\n","Requirement already satisfied: pyparsing\u003e=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib-\u003ewordcloud) (3.1.1)\n","Requirement already satisfied: python-dateutil\u003e=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib-\u003ewordcloud) (2.8.2)\n","Requirement already satisfied: six\u003e=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil\u003e=2.7-\u003ematplotlib-\u003ewordcloud) (1.16.0)\n"]}],"source":["!pip install requests\n","!pip install wordcloud\n","!pip install -q kaggle"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":1834,"status":"ok","timestamp":1710082445253,"user":{"displayName":"Bernard Cheng","userId":"09763001268899605911"},"user_tz":-480},"id":"PwExT3qiBSJn"},"outputs":[],"source":["import os\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import json\n","\n","import requests\n","from bs4 import BeautifulSoup\n","import re\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from wordcloud import WordCloud, STOPWORDS\n","import zipfile\n","from google.colab import files\n","import gzip"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":56},"id":"pkoNdmdlCquu"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'upload your Kaggle profile file now:'"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","     \u003cinput type=\"file\" id=\"files-0674c5ca-66d9-4ef5-a751-f6070d65aa7e\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" /\u003e\n","     \u003coutput id=\"result-0674c5ca-66d9-4ef5-a751-f6070d65aa7e\"\u003e\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      \u003c/output\u003e\n","      \u003cscript\u003e// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) =\u003e {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable\u003c!Object\u003e} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) =\u003e {\n","    inputElement.addEventListener('change', (e) =\u003e {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) =\u003e {\n","    cancel.onclick = () =\u003e {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) =\u003e {\n","      const reader = new FileReader();\n","      reader.onload = (e) =\u003e {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position \u003c fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","\u003c/script\u003e "],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"ename":"TypeError","evalue":"'NoneType' object is not subscriptable","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-8-195746c7b500\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 8\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"upload your Kaggle profile file now:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 8\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' mkdir ~/.kaggle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' cp kaggle.json ~/.kaggle/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m   \"\"\"\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 69\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    161\u001b[0m   \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_collections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 163\u001b[0;31m   \u001b[0;32mwhile\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'complete'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m     result = _output.eval_js(\n\u001b[1;32m    165\u001b[0m         'google.colab._files._uploadFilesContinue(\"{output_id}\")'.format(\n","\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"]}],"source":["# take a look at this post on how to download data from Kaggle to Colab\n","# https://www.kaggle.com/discussions/general/74235\n","\n","# You could use the following steps, but your Kaggle profile should be uploaded when prompted\n","\n","display(\"upload your Kaggle profile file now:\")\n","\n","files.upload()\n","! mkdir ~/.kaggle\n","! cp kaggle.json ~/.kaggle/\n","! chmod 600 ~/.kaggle/kaggle.json\n","\n","display(\"Start getting the data file from Kaggle...\")\n","\n","# How to get the dataset address: copy API command to download the dataset: go to the top-right corner three-points button, and click it -\u003e choose \"Copy API Command\" then copy it here, not forget to add ! at the beginning\n","!kaggle datasets download -d Cornell-University/arxiv\n","zip_ref = zipfile.ZipFile('arxiv.zip', 'r')\n","zip_ref.extractall('/content')\n","zip_ref.close()\n","\n","display(\"Finish getting the data file and extracting it from Kaggle.\")\n","display(\"Start getting the data file from github...\")\n","\n","!wget https://github.com/mattbierbaum/arxiv-public-datasets/releases/download/v0.2.0/internal-references-v0.2.0-2019-03-01.json.gz\n","\n","display(\"Finish getting the data files from Kaggle and Github. Both data files are stored in session under /content\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"fzS4KmfGO5CC"},"outputs":[],"source":["# this step is necessary if you want to save the transformed data into google drive and reuse.\n","from google.colab import drive\n","drive.mount(\"/content/drive\")\n"]},{"cell_type":"markdown","metadata":{"id":"iNfht3SiBSJo"},"source":["# Part 1 - preprocessing the data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"1Qug68InBSJo"},"outputs":[],"source":["project_path = r\"/content/drive/MyDrive/Colab Notebooks/nus-cs5246-master-text-mining/final_project\"\n","project_data_path = project_path + \"/data\"\n","if not os.path.exists(project_path + \"/data\"):\n","    os.makedirs(project_path + '/data/')\n","\n","metadata_file = \"/content/arxiv-metadata-oai-snapshot.json\"\n","ref_file = \"/content/internal-references-v0.2.0-2019-03-01.json.gz\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"-RhbtW4EBSJo"},"outputs":[],"source":["\n","with gzip.open(ref_file, \"r\") as f:\n","    citations = json.load(f)\n","\n","with open(project_data_path + \"/arxiv-metadata-ext-citation.csv\",\"w+\") as f_out :\n","    f_out.write(\"id,id_reference\\n\")\n","    for i,id in enumerate(citations):\n","        if i % 50000 == 0:\n","            print(\"Processing the citation records from {} to {}\".format(i, i+ 50000))\n","\n","        for k in citations[id]:\n","            f_out.write(f'{id},{k}\\n')\n","\n","print(\"Finish processing all the internal citation and save it to {}\".format(\"data/arxiv-metadata-ext-citation.csv\"))\n","\n","# uncomment the below code if you want to check the data\n","# df_citations = pd.read_csv(project_data_path + \"/arxiv-metadata-ext-citation.csv\",dtype={\"id\":object,\"id_reference\":object})\n","# df_citations.head()\n","# print(\"total records of df:citations:\", df_citations.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ZFjmUKhoBSJp"},"outputs":[],"source":["## reorganize the format of the categories for each record\n","\n","with open(project_data_path + \"/arxiv-metadata-ext-category.csv\",\"w+\") as f_out :\n","    f_out.write(\"id,category_id\\n\")\n","\n","    with open(metadata_file) as f_in:\n","        for i,line in enumerate(f_in):\n","            if i % 100000 == 0:\n","                print(\"Processing the categories of records from {} to {}...\".format(i, i + 100000))\n","\n","            row = json.loads(line)\n","            id = row[\"id\"]\n","            categories = row[\"categories\"].split()\n","            for c in categories:\n","                f_out.write ( f'\"{id}\",\"{c}\"\\n'  )\n","print(\"Finish processing the categories of records and save it to {}\".format(project_data_path + \"/arxiv-metadata-ext-category.csv\"))\n","\n","# uncomment the below code if you want to check the data\n","# df_categories = pd.read_csv(\"data/arxiv-metadata-ext-category.csv\",dtype={\"id\":object,\"category_id\":object})\n","# df_categories.head()\n","# print(\"total records of df:citations:\", df_categories.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"DlGBeqx4BSJp"},"outputs":[],"source":["#################    DO NOT RUN THIS CELL, just For Your Reference     ##############################\n","################ This cell includes the original method  on Keggle to organize ######################\n","################ meta data, but the info is too huge and it could not save to  ######################\n","################                        csv file.                              ######################\n","## create a metadata paper csv\n","## This cell is just for reference. If we combine all the data into one csv file, we will run out of memory. We need to separate\n","## the files by year. See the next cell and this cell's result could be deleted.\n","\n","\n","# METADATA_PATH = r'archive/arxiv-metadata-oai-snapshot.json'\n","\n","\n","# titles = []\n","# abstracts = []\n","# ids = []\n","# authors = []\n","# journal_refs = []\n","# licenses = []\n","\n","\n","# with open(METADATA_PATH) as f_in:\n","#     for i,line in enumerate(f_in):\n","#         if i % 50000 == 0:\n","#             print(\"Start processing record from {} to {}...\".format(i, i+50000))\n","\n","#         row = json.loads(line)\n","\n","#         titles.append(row[\"title\"])\n","#         abstracts.append(row[\"abstract\"])\n","#         ids.append(row[\"id\"])\n","#         authors.append(row[\"authors\"])\n","#         journal_refs.append(row[\"journal-ref\"])\n","#         licenses.append(row[\"license\"])\n","\n","#     print(\"Finish processing all the papers. Total records: {}\".format(i))\n","\n","# # the dataframe is too big and could not save to csv\n","# df_papers = pd.DataFrame({\n","#     'id' : ids,\n","#     'title' : titles,\n","#     'abstract' : abstracts,\n","#     'authors' : authors,\n","#     'journal-ref' : journal_refs,\n","#     'license':licenses\n","\n","# })\n","# df_papers.to_csv(\"arxiv-metadata-ext-paper.csv\", index=False)\n","# df_papers.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"d5kYsiFFBSJp"},"outputs":[],"source":["###### separate the original meta file to different files by year\n","# This cell just creates two functions to save to .csv or .xlsx\n","\n","def saveToFile(year_value, file_name, ids, titles, abstracts, authors, journal_refs, years, licenses):\n","    df_papers = pd.DataFrame({\n","    'id' : ids[year_value],\n","    'title' : titles[year_value],\n","    'abstract' : abstracts[year_value],\n","    'authors' : authors[year_value],\n","    'journal-ref' : journal_refs[year_value],\n","    'year': years[year_value],\n","    'license':licenses[year_value]})\n","\n","    df_papers.to_csv(file_name, index=False)\n","\n","def saveToExcelFile(year_value, file_name, ids, titles, abstracts, authors, journal_refs, years, licenses):\n","    df_papers = pd.DataFrame({\n","    'id' : ids[year_value],\n","    'title' : titles[year_value],\n","    'abstract' : abstracts[year_value],\n","    'authors' : authors[year_value],\n","    'journal-ref' : journal_refs[year_value],\n","    'year': years[year_value],\n","    'license':licenses[year_value]})\n","\n","    df_papers.to_excel(file_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"75nd0_56BSJp"},"outputs":[],"source":["## save metadata data and then save them to different .csv\n","\n","import json\n","import pandas as pd\n","\n","def get_metadata():\n","    with open(metadata_file, 'r') as f:\n","        for line in f:\n","            yield line\n","\n","# create a dictionary structure for each piece of meta data and later on, we could save the value by year\n","titles = {}\n","abstracts = {}\n","ids = {}\n","authors = {}\n","journal_refs = {}\n","licenses = {}\n","years = {}\n","for year in range(1999, 2025):\n","    titles[year] = []\n","    abstracts[year] = []\n","    ids[year] = []\n","    authors[year] = []\n","    journal_refs[year] = []\n","    licenses[year] = []\n","    years[year] = []\n","\n","index = 0\n","records_21st = 0\n","metadata = get_metadata()\n","\n","print(\"Start processing the meta data...\\n\")\n","\n","for paper in metadata:\n","    index += 1\n","    # each one item is a paper info.\n","    paper_dict = json.loads(paper)\n","\n","    try:\n","        year = int(paper_dict['versions'][0]['created'].split()[3])\n","\n","        if year \u003c 2000:\n","            year = 1999\n","        else:\n","            records_21st +=1\n","\n","        years[year].append(year)\n","        titles[year].append(paper_dict[\"title\"])\n","        abstracts[year].append(paper_dict[\"abstract\"])\n","        ids[year].append(paper_dict[\"id\"])\n","        authors[year].append(paper_dict[\"authors\"])\n","        journal_refs[year].append(paper_dict[\"journal-ref\"])\n","        licenses[year].append(paper_dict[\"license\"])\n","    except:\n","        print(\"error when handling paper id {}\".format(paper_dict[\"id\"]))\n","        print(paper_dict)\n","        # break\n","\n","    if index % 50000 == 0:\n","        print(\"Start processing index {} to {}...\".format(index, index+50000))\n","\n","print(\"Finish processing the papers. Totally there are {} records. {} records are after 2000.\".format(index, records_21st))\n","\n","######## This step is necessary, because we need to open those csv files in the later steps ##########\n","print(\"Now save to files by year...\")\n","\n","for i in range(1999, 2025):\n","    if len(ids) \u003e 0:\n","        saveToFile(i, project_data_path + \"/output_\" + str(i) + \".csv\", ids, titles, abstracts, authors, journal_refs, years, licenses)\n","\n","        if i == 1999:\n","            print(\"Finish saving to the file {}.\".format(project_data_path + \"/output_\" + str(i) + \".csv for all the records before 2000.\"))\n","        else:\n","            print(\"Finish saving to the file {}.\".format(project_data_path + \"/output_\" + str(i) + \".csv\"))\n","\n","print(\"Finish saving all the files.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"eOxK52phBSJq"},"outputs":[],"source":["######################### optional ###################################################################\n","########### if you want to check data more conviently with excel file format, you could run ##########\n","for i in range(2001, 2025):\n","    print(\"Start saving to the file {}.\".format(project_data_path + \"/output_\" + str(i) + \".xlsx\"))\n","    saveToExcelFile(i, \"data/output_\" + str(i) + \".xlsx\", ids, titles, abstracts, authors, journal_refs, years, licenses)\n","print(\"Finish saving all the files to xlsx.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"S97HkBxmBSJq"},"outputs":[],"source":["\n","######## This cell will save each paper's version 1 info to a file. It's running a bit slow...\n","\n","## create a metadata version csv\n","with open(project_data_path + \"/arxiv-metadata-ext-version.csv\",\"w+\") as f_out :\n","    f_out.write(\"id,year,month\\n\")\n","\n","    with open(metadata_file) as f_in:\n","        for i,line in enumerate(f_in):\n","            if i % 100000 == 0:\n","                print(\"Start processing the version of records from {} to {}\".format(i, i + 100000))\n","\n","\n","            row = json.loads(line)\n","            id = row[\"id\"]\n","            date_value = pd.to_datetime(row[\"versions\"][0]['created'])\n","            month = date_value.month\n","            year = date_value.year\n","\n","            f_out.write (f'{id},{year},{month}\\n')\n","\n","print(\"Finish processing the versions of records and save it to {}\".format(project_data_path + \"/arxiv-metadata-ext-version.csv\"))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"3RRvJOnXBSJq"},"outputs":[],"source":["## load taxonomy from https://arxiv.org/category_taxonomy\n","website_url = requests.get('https://arxiv.org/category_taxonomy').text\n","soup = BeautifulSoup(website_url,'lxml')\n","\n","root = soup.find('div',{'id':'category_taxonomy_list'})\n","\n","tags = root.find_all([\"h2\",\"h3\",\"h4\",\"p\"], recursive=True)\n","\n","level_1_name = \"\"\n","level_2_code = \"\"\n","level_2_name = \"\"\n","\n","level_1_names = []\n","level_2_codes = []\n","level_2_names = []\n","level_3_codes = []\n","level_3_names = []\n","level_3_notes = []\n","\n","for t in tags:\n","    if t.name == \"h2\":\n","        level_1_name = t.text\n","        level_2_code = t.text\n","        level_2_name = t.text\n","    elif t.name == \"h3\":\n","        raw = t.text\n","        level_2_code = re.sub(r\"(.*)\\((.*)\\)\",r\"\\2\",raw)\n","        level_2_name = re.sub(r\"(.*)\\((.*)\\)\",r\"\\1\",raw)\n","    elif t.name == \"h4\":\n","        raw = t.text\n","        level_3_code = re.sub(r\"(.*) \\((.*)\\)\",r\"\\1\",raw)\n","        level_3_name = re.sub(r\"(.*) \\((.*)\\)\",r\"\\2\",raw)\n","    elif t.name == \"p\":\n","        notes = t.text\n","        level_1_names.append(level_1_name)\n","        level_2_names.append(level_2_name)\n","        level_2_codes.append(level_2_code)\n","        level_3_names.append(level_3_name)\n","        level_3_codes.append(level_3_code)\n","        level_3_notes.append(notes)\n","\n","df_taxonomy = pd.DataFrame({\n","    'group_name' : level_1_names,\n","    'archive_name' : level_2_names,\n","    'archive_id' : level_2_codes,\n","    'category_name' : level_3_names,\n","    'category_id' : level_3_codes,\n","    'category_description': level_3_notes\n","})\n","df_taxonomy.to_csv(project_data_path + \"/arxiv-metadata-ext-taxonomy.csv\", index=False)\n","print(\"Finish processing the taxonomy of records and save it to {}\".format(project_data_path + \"/arxiv-metadata-ext-taxonomy.csv\"))\n","\n","df_taxonomy.groupby([\"group_name\",\"archive_name\"]).head(3)\n"]},{"cell_type":"markdown","metadata":{"id":"g3jkGfraBSJq"},"source":["# Part 2\n","*************************************************************************************************\n","**Till this point, we have finished pre-processing data and store them under /data subfolder.**\n","\n","* The following steps are independent from the above steps *\n","*************************************************************************************************"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"6VpI1PODBSJq"},"outputs":[],"source":["######################### start to read from the pre-processed documents ##############################\n","\n","df_citations = pd.read_csv(project_data_path + \"/arxiv-metadata-ext-citation.csv\",dtype={\"id\":object,\"id_reference\":object})\n","df_citations.head()\n","print(\"total records of df:citations:\", df_citations.shape)\n","\n","df_categories = pd.read_csv(project_data_path + \"/arxiv-metadata-ext-category.csv\",dtype={\"id\":object,\"category_id\":object})\n","df_categories.head()\n","print(\"total records of df:categories:\", df_categories.shape)\n","\n","df_taxonomy = pd.read_csv(project_data_path + \"/arxiv-metadata-ext-taxonomy.csv\")\n","df_taxonomy.head()\n","print(\"total records of df_taxonomy:\",df_taxonomy.shape)\n","\n","df_versions = pd.read_csv(project_data_path + \"/arxiv-metadata-ext-version.csv\", dtype={'id': object})\n","df_versions.head()\n","print(\"total records of df_versions:\",df_versions.shape)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"RFfHExSMBSJq"},"outputs":[],"source":["###### this cell defines some functions\n","\n","def count_by_category_and_year(group_name):\n","    cats = df_categories.merge(df_taxonomy, on=\"category_id\").query(\"group_name == @group_name\").merge(df_versions[[\"id\",\"year\"]], on =\"id\")\n","    cats = cats.groupby([\"year\",\"category_name\"]).count().reset_index().pivot(index=\"category_name\", columns=\"year\",values=\"id\")\n","    return cats\n","\n","def count_by_archive_and_year(group_name):\n","    cats = df_categories.merge(df_taxonomy, on=\"category_id\").query(\"group_name == @group_name\").merge(df_versions[[\"id\",\"year\"]], on =\"id\")\n","    cats = cats.groupby([\"year\",\"archive_name\"]).count().reset_index().pivot(index=\"archive_name\", columns=\"year\",values=\"id\")\n","    return cats\n","\n","def show_count_by_category_and_year(group_name,figsize=(20,10)):\n","    plt.figure(figsize=figsize)\n","    plt.title(f\"{group_name} papers by category and year\")\n","    sns.heatmap(count_by_category_and_year(group_name),cmap=\"Greens\", linewidths=0.01, linecolor='palegreen')\n","    plt.show()\n","\n","def show_count_by_archive_and_year(group_name=\"Physics\",figsize=(10,5)):\n","    plt.figure(figsize=figsize)\n","    plt.title(f\"{group_name} papers by archive and year\")\n","    sns.heatmap(count_by_archive_and_year(group_name),cmap=\"Greens\", linewidths=0.01, linecolor='palegreen')\n","    plt.show()\n","\n","    # updated from the original Kaggle code\n","def top_k_influential (group_name, top_k=5, threshold=100):\n","    print(\"Calculating top influential papers. Take some time ...\")\n","    # NOTE: the current algorithm will put one paper into one or more categories. Under this condition, one citation of a paper\n","    # which falls into 1+ categories will be calculated multiple times!!! It is designed like this\n","    ids = df_categories.merge(df_taxonomy, on=\"category_id\").query(\"group_name ==@group_name\").drop_duplicates([\"id\",\"group_name\"], inplace=False)[\"id\"].values\n","    cits = df_citations.query('id.isin(@ids)', engine=\"python\").merge(df_versions[[\"id\",\"year\"]], on =\"id\").groupby([\"year\",\"id_reference\"]).count()\n","    cits = cits.reset_index()\n","    cits.columns = ['year', 'id', 'citation_count']\n","    cits = cits.loc[cits.groupby('year')['citation_count'].nlargest(top_k).reset_index()['level_1']]\n","    cits = cits.query(\"citation_count \u003e @threshold\")\n","    return cits\n","\n","\n","    # newly added function\n","def get_paper_info(cits):\n","    id_ls = cits['id'].values\n","    meta_df = pd.DataFrame()\n","    for year in range(1999, 2022):\n","        df = pd.read_csv(project_data_path + \"/output_\" + str(year) + \".csv\", dtype={\"id\":object})\n","        print(\"Check {} data which contains the paper id...\".format(year))\n","        if year == 1999:\n","            meta_df = df[df['id'].isin(id_ls)]\n","        else:\n","            meta_df = pd.concat([meta_df, df[df['id'].isin(id_ls)]], axis = 0)\n","\n","    cits = cits.merge(meta_df, how='left', on='id')\n","    cits = cits.rename(columns={\"year_x\":\"citation_year\", \"year_y\": \"first_publish_year\"})\n","    return cits\n","\n","    # updated from the original Kaggle code\n","def show_influential_heatmap (group_name, cits, figsize=(8,6)):\n","    # we have two years related to the paper. Its citation year and its publishing year.\n","    # at this moment, we use its citation_year to form heatmap\n","    # we have changed id_reference to id in function top_k_influential() so Values = \"id\"\n","    hm_cits =  cits.pivot(index=\"title\", columns=\"citation_year\",values=\"citation_count\")\n","\n","    plt.figure(figsize=figsize)\n","    plt.title(\"Top influential papers by year\")\n","    sns.heatmap(hm_cits,cmap=\"Greens\", linewidths=0.01, linecolor='palegreen')\n","    plt.show()\n","\n","def make_clickable(val):\n","    # target _blank to open new window\n","    return '\u003ca target=\"_blank\" href=\"{}\"\u003e{}\u003c/a\u003e'.format(val, val)\n","\n","def show_influential_table ( cits):\n","    df = cits.groupby([\"id\",\"title\",\"authors\",\"abstract\"]).agg({\"citation_count\":\"sum\"}).reset_index()\n","    df = df.sort_values(by=\"citation_count\",ascending = False).reset_index(drop=True)\n","    df [\"url\"] = df[\"id\"].map(lambda x:  f'https://arxiv.org/pdf/{x}' )\n","    df [\"authors\"] = df[\"authors\"].map(lambda x: x if len(str(x)) \u003c 50 else str(x)[:47] + \"...\" )\n","\n","    df =df [[\"title\",\"authors\",\"abstract\",\"url\",\"citation_count\"]]\n","    return df.style.format({'url': make_clickable})\n","\n","# Thanks : https://www.kaggle.com/aashita/word-clouds-of-various-shapes ##\n","def plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0),\n","                   title = None, title_size=40, image_color=False):\n","    stopwords = set(STOPWORDS)\n","    more_stopwords = {'We', 'paper', 'new'}\n","    stopwords = stopwords.union(more_stopwords)\n","\n","    wordcloud = WordCloud(background_color='black',\n","                    stopwords = stopwords,\n","                    max_words = max_words,\n","                        max_font_size = max_font_size,\n","                    random_state = 42,\n","                    width=800,\n","                    height=400,\n","                    mask = mask,\n","                    min_word_length = 4,\n","                    #normalize_plurals = True,\n","                    #collocations = True,\n","                       #collocation_threshold = 10\n","                         )\n","    wordcloud.generate(str(text))\n","\n","    plt.figure(figsize=figure_size)\n","    if image_color:\n","        image_colors = wordcloud.ImageColorGenerator(mask);\n","        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n","        plt.title(title, fontdict={'size': title_size,\n","                                  'verticalalignment': 'bottom'})\n","    else:\n","\n","        plt.imshow(wordcloud);\n","        plt.title(title, fontdict={'size': title_size, 'color': 'black',\n","                                  'verticalalignment': 'bottom'})\n","        plt.axis('off');\n","        plt.tight_layout()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"jJtYZJDvBSJr"},"outputs":[],"source":["\n","############## Copy from the Keggle project ###############\n","# check the publication per monthly\n","df = df_versions.groupby([\"year\",\"month\"]).agg({\"id\":'count'}).reset_index()\n","df[\"tot\"] = df[\"id\"].cumsum()\n","\n","df = df.query(\"year \u003e 2000 and ( year \u003c 2025)\")\n","df[\"month\"] =  df[\"year\"].astype(str) + \"-\" + df[\"month\"].astype(str)\n","display(df.head())\n","\n","# group by group_name and calculate the total paper amount per group\n","_df = df_categories.merge(df_taxonomy, on=\"category_id\").drop_duplicates([\"id\",\"group_name\"]).groupby(\"group_name\").agg({\"id\":\"count\"}).sort_values(by=\"id\",ascending=False).reset_index()\n","# id and category_id is different. id refers to paper's unique id, category_id is a category\n","_df.columns = ['group_name', 'paper_amount']\n","display(_df.head())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"nqU9VC86BSJr"},"outputs":[],"source":["#### copy Kaggle code. Visualize the data\n","\n","fig = plt.figure(figsize=(20,10))\n","ax1 = plt.subplot2grid((2, 2), (0, 0))\n","ax1.title.set_text('ArXiv papers')\n","ax1.plot(df[\"month\"], df[\"tot\"])\n","ax1.hlines(y=1e6, xmin=0, xmax=len(df), color='green', linestyle=\"dotted\")\n","ax1.hlines(y=1.5e6, xmin=0, xmax=len(df), color='green', linestyle=\"dotted\")\n","ax1.set_xticks(np.arange(0, len(df), 12.0))\n","ax1.tick_params('x',labelrotation=90)\n","\n","ax2 = plt.subplot2grid((2, 2), (1, 0))\n","ax2.title.set_text(\"ArXiv papers by month\")\n","ax2.plot(df[\"month\"], df[\"id\"])\n","ax2.hlines(y=10000, xmin=0, xmax=len(df), color='green', linestyle=\"dotted\")\n","ax2.hlines(y=15000, xmin=0, xmax=len(df), color='green', linestyle=\"dotted\")\n","ax2.set_xticks(np.arange(0, len(df), 12.0))\n","ax2.tick_params('x',labelrotation=90)\n","\n","ax3 = plt.subplot2grid((2, 2), (0, 1), rowspan=2)\n","ax3.title.set_text(\"ArXiv papers by group\")\n","explode = (0, 0, 0, 0.2, 0.3, 0.3, 0.2, 0.1)\n","ax3.pie(_df[\"paper_amount\"],  labels=_df[\"group_name\"], autopct='%1.1f%%', startangle=160, explode=explode)\n","\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"sCDfZgD4BSJr"},"outputs":[],"source":["\n","# Claudia: copy Kaggle data visualization. Change something here. df_versions dataframe only contains version 1 data, so no need to store field \"version\"\n","cats = df_categories.merge(df_taxonomy, on=\"category_id\").merge(df_versions[[\"id\",\"year\"]], on =\"id\")\n","cats = cats.groupby([\"year\",\"group_name\"]).count().reset_index().pivot(index=\"group_name\", columns=\"year\",values=\"id\")\n","\n","# Claudia: heatmap shows the relationship between year, group_name and paper publishing amount\n","plt.figure(figsize=(10,5))\n","plt.title(\"ArXiv papers by group and year\")\n","sns.heatmap(cats,cmap=\"Greens\", linewidths=0.01, linecolor='palegreen')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"HGlX9C7tBSJr"},"outputs":[],"source":["# Kaggle data visualization, show year, category and publishing amount relationship\n","group_name=\"Physics\"\n","show_count_by_archive_and_year(group_name)\n","\n","# for Computer Science, show_count_by_category_and_year because at archive level, there is no diffence\n","group_name=\"Computer Science\"\n","show_count_by_category_and_year(group_name)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"hRXa3CyBBSJr"},"outputs":[],"source":["# Kaggle data visualization. Show the top k influential papers. it uses the citation number as the\n","# measurement. Pls refer to method top_k_influential\n","group_name = \"Physics\"\n","cits = top_k_influential (group_name, top_k=3)\n","cits = get_paper_info(cits)\n","show_influential_heatmap (group_name, cits=cits, figsize=(8,6))\n","show_influential_table (cits)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"qGhydXxaBSJr"},"outputs":[],"source":["group_name = \"Computer Science\"\n","cits = top_k_influential (group_name, top_k=5)\n","cits = get_paper_info(cits)\n","show_influential_heatmap (group_name, cits=cits, figsize=(6,4))\n","show_influential_table (cits)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"lrROhRuMBSJr"},"outputs":[],"source":["group_name=\"Electrical Engineering and Systems Science\"\n","\n","show_count_by_category_and_year (group_name,figsize=(8,6))\n","cits = top_k_influential (group_name, top_k=1, threshold=1)\n","cits = get_paper_info(cits)\n","show_influential_heatmap (group_name, cits=cits, figsize=(6,4))\n","show_influential_table (cits)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"EwkBIx-qBSJs"},"outputs":[],"source":["group_name=\"Quantitative Finance\"\n","\n","show_count_by_category_and_year (group_name,figsize=(10,4))\n","cits = top_k_influential (group_name, top_k=1, threshold=3)\n","cits = get_paper_info(cits)\n","show_influential_heatmap (group_name, cits=cits, figsize=(10,3))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"5OIq5BQxBSJs"},"outputs":[],"source":["df_specialCase = df_citations.query(\"id_reference == '1412.6980'\")\\\n",".merge(df_categories,on=\"id\")\\\n",".merge(df_taxonomy,on=\"category_id\").drop_duplicates([\"id\",\"group_name\"])\\\n",".merge(df_versions[[\"id\",\"year\"]], on =\"id\")\n","\n","hmap =df_specialCase.groupby([\"group_name\",\"year\"]).agg({\"id\":\"count\"}).reset_index().pivot(index=[\"group_name\"], columns=\"year\",values=\"id\")\n","\n","plt.figure(figsize=(10,5))\n","plt.title(\"Papers that reference 'Adam: A Method for Stochastic Optimization'\")\n","sns.heatmap(hmap,cmap=\"Greens\", linewidths=0.01, linecolor='palegreen', annot=True, fmt=\".0f\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"uTnpOq3vBSJs"},"outputs":[],"source":[]}],"metadata":{"colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}